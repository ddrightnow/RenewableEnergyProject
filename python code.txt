python code

To create an environment, use conda create -n env_name list of packages in your terminal. Here -n env_name sets the name of your environment (-n for name) and list of packages is the list of packages you want installed in the environment. For example, to create an environment named my_env and install numpy in it, type conda create -n my_env numpy

ou can specify which version of Python to install in the environment. This is useful when you're working with code in both Python 2.x and Python 3.x. To create an environment with a specific Python version, do something like conda create -n py3 python=3 or conda create -n py2 python=2

Entering an environment
On Windows, use activate my_env

check this out with conda list. Installing packages in the environment is the same as before: conda install package_name. Only this time, the specific packages you install will only be available when you're in the environment. To leave the environment, type source deactivate (on OSX/Linux). On Windows, use deactivate

Saving and loading environments
A really useful feature is sharing environments so others can install all the packages used in your code, with the correct versions. You can save the packages to a YAML file with conda env export > environment.yaml. The first part conda env export writes out all the packages in the environment, including the Python version.

Above you can see the name of the environment and all the dependencies (along with versions) are listed. The second part of the export command, > environment.yaml writes the exported text to a YAML file environment.yaml. This file can now be shared and others will be able to create the same environment you used for the project.

To create an environment from an environment file use conda env create -f environment.yaml. This will create a new environment with the same name listed in environment.yaml.

Listing environments
If you forget what your environments are named (happens to me sometimes), use conda env list to list out all the environments you've created. You should see a list of environments, there will be an asterisk next to the environment you're currently in. The default environment, the environment used when you aren't in one, is called root.

Removing environments
If there are environments you don't use anymore, conda env remove -n env_name will remove the specified environment (here, named env_name).

=========
Best practices
Using environments
One thing that’s helped me tremendously is having separate environments for Python 2 and Python 3. I used conda create -n py2 python=2 and conda create -n py3 python=3 to create two separate environments, py2 and py3. Now I have a general use environment for each Python version. In each of those environments, I've installed most of the standard data science packages (numpy, scipy, pandas, etc.). Remember that when you set up an environment initially, you'll only start with the standard packages and whatever packages you specify in your conda create statement.

I’ve also found it useful to create environments for each project I’m working on. It works great for non-data related projects too like web apps with Flask. For example, I have an environment for my personal blog using Pelican.

Sharing environments
When sharing your code on GitHub, it's good practice to make an environment file and include it in the repository. This will make it easier for people to install all the dependencies for your code. I also usually include a pip requirements.txt file using pip freeze (learn more here) for people not using conda.
============
Use + to concatenate lists.
Use `*` to repeat lists.
Use the in operator to check if something is inside a list.


==
regressio analysis

y = melbourne_data.Price
melbourne_predictors = ['Rooms', 'Bathroom', 'Landsize', 'BuildingArea', 'YearBuilt', 'Lattitude', 'Longtitude']
X = melbourne_data[melbourne_predictors]
# Define model
melbourne_model = DecisionTreeRegressor()

# Fit model
melbourne_model.fit(X, y)
print("Making predictions for the following 5 houses:")
print(X.head())
print("The predictions are")
print(melbourne_model.predict(X.head()))
==
from sklearn.metrics import mean_absolute_error
from sklearn.model_selection import train_test_split

# split data into training and validation data, for both predictors and target
# The split is based on a random number generator. Supplying a numeric value to
# the random_state argument guarantees we get the same split every time we
# run this script.

train_X, val_X, train_y, val_y = train_test_split(X, y,random_state = 0)
# Define model
melbourne_model = DecisionTreeRegressor()
# Fit model
melbourne_model.fit(train_X, train_y)

# get predicted prices on validation data
val_predictions = melbourne_model.predict(val_X)
print(mean_absolute_error(val_y, val_predictions))
MAE - On average, our predictions are off by about X

from sklearn.metrics import mean_absolute_error
from sklearn.tree import DecisionTreeRegressor

def get_mae(max_leaf_nodes, predictors_train, predictors_val, targ_train, targ_val):
    model = DecisionTreeRegressor(max_leaf_nodes=max_leaf_nodes, random_state=0)
    model.fit(predictors_train, targ_train)
    preds_val = model.predict(predictors_val)
    mae = mean_absolute_error(targ_val, preds_val)
    return(mae)
=
Random Forests
forest_model = RandomForestRegressor()
forest_model.fit(train_X, train_y)
melb_preds = forest_model.predict(val_X)
print(mean_absolute_error(val_y, melb_preds))

TEST DATA
import numpy as np
import pandas as pd
from sklearn.ensemble import RandomForestRegressor

# Read the data
train = pd.read_csv('../input/train.csv')

# pull data into target (y) and predictors (X)
train_y = train.SalePrice
predictor_cols = ['LotArea', 'OverallQual', 'YearBuilt', 'TotRmsAbvGrd']

# Create training predictors data
train_X = train[predictor_cols]

my_model = RandomForestRegressor()
my_model.fit(train_X, train_y)

# Read the test data
test = pd.read_csv('../input/test.csv')
# Treat the test data in the same way as training data. In this case, pull same columns.
test_X = test[predictor_cols]
# Use the model to make predictions
predicted_prices = my_model.predict(test_X)
# We will look at the predicted prices to ensure we have something sensible.
print(predicted_prices)

===
Prepare Submission File

We make submissions in CSV files. Your submissions usually have two columns: an ID column and a prediction column. The ID field comes from the test data (keeping whatever name the ID field had in that data, which for the housing data is the string 'Id'). The prediction column will use the name of the target field.

We will create a DataFrame with this data, and then use the dataframe's to_csv method to write our submission file. Explicitly include the argument index=False to prevent pandas from adding another column in our csv file

my_submission = pd.DataFrame({'Id': test.Id, 'SalePrice': predicted_prices})
# you could use any filename. We choose submission here
my_submission.to_csv('submission.csv', index=False)
===

1) A Simple Option: Drop Columns with Missing Values¶

ou'll have both a training dataset and a test dataset. You will want to drop the same columns in both DataFrames. In that case, you would write

cols_with_missing = [col for col in original_data.columns 
                                 if original_data[col].isnull().any()]
redued_original_data = original_data.drop(cols_with_missing, axis=1)
reduced_test_data = test_data.drop(cols_with_missing, axis=1)

=
2) A Better Option: Imputation
Imputation fills in the missing value with some number. The imputed value won't be exactly right in most cases, but it usually gives more accurate models than dropping the column entirely.

This is done with

from sklearn.preprocessing import Imputer
my_imputer = Imputer()
data_with_imputed_values = my_imputer.fit_transform(original_data)
=
# make copy to avoid changing original data (when Imputing)
new_data = original_data.copy()

# make new columns indicating what will be imputed
cols_with_missing = (col for col in new_data.columns 
                                 if new_data[c].isnull().any())
for col in cols_with_missing:
    new_data[col + '_was_missing'] = new_data[col].isnull()

# Imputation
my_imputer = Imputer()
new_data = my_imputer.fit_transform(new_data)
\\
df.isnull().any() generates a boolean array (True if the column has a missing value, False otherwise). You can use it to index into df.columns:

df.columns[df.isnull().any()]
will return a list of the columns which have missing values.
=

Basic Problem Set-up
In [1]:
import pandas as pd

# Load data
melb_data = pd.read_csv('../input/melbourne-housing-snapshot/melb_data.csv')

from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_error
from sklearn.model_selection import train_test_split

melb_target = melb_data.Price
melb_predictors = melb_data.drop(['Price'], axis=1)

# For the sake of keeping the example simple, we'll use only numeric predictors. 
melb_numeric_predictors = melb_predictors.select_dtypes(exclude=['object'])
=
y = melb_target
X = melb_data[melb_numeric_predictors2]
print(7*8)
#print(melb_numeric_predictors.columns)

X_train, X_test, y_train, y_test = train_test_split(X, y,random_state = 0) 
#score_dataset(X_train, X_test, y_train, y_test) 

=====

Best Prepare Your Data For Naive Bayes
Categorical Inputs: Naive Bayes assumes label attributes such as binary, categorical or nominal.
Gaussian Inputs: If the input variables are real-valued, a Gaussian distribution is assumed. In which case the algorithm will perform better if the univariate distributions of your data are Gaussian or near-Gaussian. This may require removing outliers (e.g. values that are more than 3 or 4 standard deviations from the mean).
Classification Problems: Naive Bayes is a classification algorithm suitable for binary and multiclass classification.
Log Probabilities: The calculation of the likelihood of different class values involves multiplying a lot of small numbers together. This can lead to an underflow of numerical precision. As such it is good practice to use a log transform of the probabilities to avoid this underflow.
Kernel Functions: Rather than assuming a Gaussian distribution for numerical input values, more complex distributions can be used such as a variety of kernel density functions.
Update Probabilities: When new data becomes available, you can simply update the probabilities of your model. This can be helpful if the data changes frequently.

Advantages and Disadvantage of Naive Bayes classifier
Advantages
Naive Bayes Algorithm is a fast, highly scalable algorithm.
Naive Bayes can be use for Binary and Multiclass classification. It provides different types of Naive Bayes Algorithms like GaussianNB, MultinomialNB, BernoulliNB.
It is a simple algorithm that depends on doing a bunch of counts.
Great choice for Text Classification problems. It’s a popular choice for spam email classification.
It can be easily train on small dataset
Disadvantages
It considers all the features to be unrelated, so it cannot learn the relationship between features. E.g., Let’s say Remo is going to a part. While cloth selection for the party, Remo is looking at his cupboard. Remo likes to wear a white color shirt. In Jeans, he likes to wear a brown Jeans, But Remo doesn’t like wearing a white shirt with Brown Jeans. Naive Bayes can learn individual features importance but can’t determine the relationship among features.

This stands in contrast to the instinctive way humans often behave, where, having concluded that they should not believe in Bookcase Aliens on the basis of the evidence in front of them, they discard that evidence entirely, denounce it, and say that it was never any evidence at all. (This is "treating arguments like soldiers" and acting like any evidence in favor of a proposition has to be "defeated.")

The Bayesian just says "yes, that is evidence in favor of the claim, but it's not quantitatively enough evidence." This idiom also stands in contrast to the practice of treating any concession an opponent makes as a victory. If true claims are supposed to have all their arguments upheld and false claims are supposed to have all their enemy arguments defeated, then a single undefeated claim of support stands as a proof of victory, no matter how strong or weak the evidence that it provides. Not so with Bayesians — a Bayesian considers the bookcase observation to be locally a piece of evidence favoring Bookcase Aliens, just massively insufficient evidence.
///////////
The Weber-Fechner law says that most human sensory perceptions are logarithmic, in the sense that a factor-of-2 intensity change feels like around the same amount of increase no matter where you are on the scale. Doubling the physical intensity of a sound feels to a human like around the same amount of change in that sound whether the initial sound was 40 decibels or 60 decibels. That's why there's an exponential decibel scale of sound intensities in the first place!

Thus the log-odds form should be, in a certain sense, the most intuitive variant of Bayes' rule to use: Just add the evidence-strength to the belief-strength! If you can make your feelings of evidence-strength and belief-strength be proportional to the logarithms of ratios, that is.

Finally, the log-odds representation gives us an even easier way to see how extraordinary claims require extraordinary evidence: If your prior belief in H is -30 bits, and you see evidence on the order of +5 bits for H, then you're going to wind up with -25 bits of belief in H, which means you still think it's far less likely than the alternatives.
///////////////
In the case of discrete inputs (indicator or frequency features for discrete events), naive Bayes classifiers form a generative-discriminative pair with (multinomial) logistic regression classifiers: each naive Bayes classifier can be considered a way of fitting a probability model that optimizes the joint likelihood {\displaystyle p(C,\mathbf {x} )} p(C,{\mathbf  {x}}), while logistic regression fits the same probability model to optimize the conditional {\displaystyle p(C\mid \mathbf {x} )} {\displaystyle p(C\mid \mathbf {x} )}
==============

you'll need str(), to convert a value into a string. str(savings), for example, will convert the float savings to a string.

Similar functions such as int(), float() and bool() will help you convert Python values into any type.

===

11. Representation Does Not Imply Learnable
Just because a function can be represented, does not mean that the function can actually be learnt. Restrictions imposed by data, time and memory, limit the functions that can actually be learnt in a feasible manner. For example, decision tree learners can not learn trees with more leaves than the number of training data points. The right question to ask is "whether a function can be learnt" and not "whether a function can be represented".

1. Learning = Representation + Evaluation + Optimization
All machine learning algorithms have three components:

Representation for a learner is the set if classifiers/functions that can be possibly learnt. This set is called hypothesis space. If a function is not in hypothesis space, it can not be learnt.
Evaluation function tells how good the machine learning model is.
Optimisation is the method to search for the most optimal learning model.
=====
matplotlib

# Change the line plot below to a scatter plot

plt.scatter(gdp_cap, life_exp)


# Put the x-axis on a logarithmic scale

plt.xscale('log')

=========*****************
sales_record = {
'price': 3.24,
'num_items': 4,
'person': 'Chris'}

sales_statement = '{} bought {} item(s) at a price of {} each for a total of {}'

print(sales_statement.format(sales_record['person'],
                             sales_record['num_items'],
                             sales_record['price'],
                             sales_record['num_items']*sales_record['price']))
*******************++++++++++++++++++++++++

# Reading and Writing CSV files
import csv

%precision 2

with open('mpg.csv') as csvfile:
    mpg = list(csv.DictReader(csvfile))
    
mpg[:3] # The first three dictionaries in our list.
==

create set to return the unique values for the number of cylinders the cars in our dataset
cylinders = set(d['cyl'] for d in mpg)
cylinders

=
more complex example where we are grouping the cars by number of cylinder, and finding the average cty mpg for each group


CtyMpgByCyl = []

for c in cylinders: # iterate over all the cylinder levels
    summpg = 0
    cyltypecount = 0
    for d in mpg: # iterate over all dictionaries
        if d['cyl'] == c: # if the cylinder level type matches,
            summpg += float(d['cty']) # add the cty mpg
            cyltypecount += 1 # increment the count
    CtyMpgByCyl.append((c, summpg / cyltypecount)) # append the tuple ('cylinder', 'avg mpg')

CtyMpgByCyl.sort(key=lambda x: x[0])
CtyMpgByCyl

=======*************
timedelta is a duration expressing the difference between two dates.


delta = dt.timedelta(days = 100) # create a timedelta of 100 days
delta
==============
# The Python Programming Language: Objects and map()
class Person:
    department = 'School of Information' #a class variable

    def set_name(self, new_name): #a method
        self.name = new_name
    def set_location(self, new_location):
        self.location = new_location
==============

class Person:
    department = 'School of Information' #a class variable

    def set_name(self, new_name): #a method 
        self.name = new_name
    def set_location(self, new_location):
        self.location = new_location
        
#To define a method, you just write it as you would have a function. The one change, is that to have access to the instance which a method is being invoked upon, you must include self, in the method signature. 
#Similarly, if you want to refer to instance variables set on the object, you prepend them with the word self, with a full stop.
#no priv or protec class #javarocks
#no need constrctor u can add __init__, still not necessary, call oject w/ person(); then add '.' notation to call its methods
=================

map() fn

But when we go to print out the map, we see that we get an odd reference value instead of a list of items that we're expecting. This is called lazy evaluation. In Python, the map function returns to you a map object. It doesn't actually try and run the function min on two items, until you look inside for a value. This is an interesting design pattern of the language, and it's commonly used when dealing with big data. This allows us to have very efficient memory management, even though something might be computationally complex. 
5:30
Maps are iterable, just like lists and tuples, so we can use a for loop to look at all of the values in the map. 
====

### The Python Programming Language: Lambda and List Comprehensions #anon fn

my_function = lambda a, b, c : a + b

my_function(1, 2, 3)

##################
You declare a lambda function with the word lambda followed by a list of arguments, followed by a colon and then a single expression and this is key. There's only one expression to be evaluated in a lambda. The expression value is returned on execution of the lambda. 
0:55
The return of a lambda is a function reference. So in this case, you would execute my_function and pass in three different parameters. 
##############

my_list = []
for number in range(0, 1000):
    if number % 2 == 0:
        my_list.append(number)
my_list

quick list creation===========
my_list = [number for number in range(0,1000) if number % 2 == 0]
my_list
=====
lowercase = 'abcdefghijklmnopqrstuvwxyz'
digits = '0123456789'

correct_answer = [a+b+c+d for a in lowercase for b in lowercase for c in digits for d in digits]

====
n = np.arange(0, 30, 2) # start at 0 count up by 2, stop before 30
n = n.reshape(3, 5) # reshape array to be 3x5
o = np.linspace(0, 4, 9) # return 9 evenly spaced values from 0 to 4
resize changes the shape and size of array in-place.
o.resize(3, 3)
np.eye(3);ones,zeros
np.diag(y)
=
Create an array using repeating list (or see np.tile)
np.array([1, 2, 3] * 3)
=np.repeat([1, 2, 3], 3)
=array([1, 1, 1, 2, 2, 2, 3, 3, 3])
=
Dot Product:x.dot(y) # dot product  1*4 + 2*5 + 3*6
=
argmax` and `argmin` return the index of the maximum and minimum values in the array.
=====================

for i, j in zip(test, test2):
    print(i,'+',j,'=',i+j)

========
Querying a Series pandas
s.iloc[2] - query by index
s.lo['golf'] - query by manually stored created index
s.loc[5]=45; adds to series s, value 4, with index 5


# Dataframe Indexing and Loading
df['Location'] = None  - addds extra column to dataframe
costs+=2   -iterate,add on a col

!cat olympics.csv - shell commad,read csv
=

df = pd.read_csv('olympics.csv', index_col = 0, skiprows=1)
index_col = 0; use first col as index; skiprows =1, use 1st row as column headers

# Querying a DataFrame
only_gold = df.where(df['Gold'] > 0)
df[(df['Gold.1'] > 0) & (df['Gold'] == 0)]

# Indexing Dataframes
df['country'] = df.index #create new col country
df = df.set_index('Gold') #set gold as index, - this call destroys the preset index gold

df = df.set_index(['STNAME', 'CTYNAME'])
df.head()

#see 2 elements/rows of a multi-labeled index df, series pandas.. -hierarchically index 
df.loc[ [('Michigan', 'Washtenaw County'),
         ('Michigan', 'Wayne County')] ]


#test
df = df.set_index([df.index, 'Name'])
df.index.names = ['Location', 'Name']
df = df.append(pd.Series(data={'Cost': 3.00, 'Item Purchased': 'Kitty Food'}, name=('Store 2', 'Kevyn')))
df

# Merging Dataframes
staff_df = pd.DataFrame([{'Name': 'Kelly', 'Role': 'Director of HR'},
                         {'Name': 'Sally', 'Role': 'Course liasion'},
                         {'Name': 'James', 'Role': 'Grader'}])
staff_df = staff_df.set_index('Name')
student_df = pd.DataFrame([{'Name': 'James', 'School': 'Business'},
                           {'Name': 'Mike', 'School': 'Law'},
                           {'Name': 'Sally', 'School': 'Engineering'}])
student_df = student_df.set_index('Name')

pd.merge(staff_df, student_df, how='outer', left_index=True, right_index=True)
how - is for merge type, left,right index for index to e used for joininh, index can be col name as below.
=
products = products.reset_index()

invoices=invoices.reset_index()
###reset index before merging dataframes
pd.merge(staff_df, student_df, how='left', left_on='Name', right_on='Name') - you can also use indices to join them


#Idiomatic Pandas: Making Code Pandorable


======
#applying a fn across a df,use applymap across a row, apply.
import numpy as np
def min_max(row):
    data = row[['POPESTIMATE2010',
                'POPESTIMATE2011',
                'POPESTIMATE2012',
                'POPESTIMATE2013',
                'POPESTIMATE2014',
                'POPESTIMATE2015']]
    return pd.Series({'min': np.min(data), 'max': np.max(data)})
#or - row['max'] = np.max(data)
    row['min'] = np.min(data)
    return row


df.apply(min_max, axis=1)
====
lambda 

rows = ['POPESTIMATE2010',
        'POPESTIMATE2011',
        'POPESTIMATE2012',
        'POPESTIMATE2013',
        'POPESTIMATE2014',
        'POPESTIMATE2015']
df.apply(lambda x: np.max(x[rows]), axis=1)

=====
# Group by
for group, frame in df.groupby('STNAME'):
    avg = np.average(frame['CENSUS2010POP'])
    print('Counties in state ' + group + ' have an average population of ' + str(avg))
==>group fn, groupby stname, avg = avg frame census2010pop, print out result
==========

df = df.set_index('STNAME')

def fun(item):
    if item[0]<'M': #all states before letter m
        return 0
    if item[0]<'Q': #all states before letter q
        return 1
    return 2

for group, frame in df.groupby(fun):
    print('There are ' + str(len(frame)) + ' records in group ' + str(group) + ' for processing.')

set index 1st, use group fn & fun (we define) fn to segmet data. typically usued to reistribute work.

===============

df.groupby('STNAME').agg({'CENSUS2010POP': np.average})
==> this is to group, then apply a fun across a column; gives the avg census.. over the states; similar to r's tapply fn.

==> finally did it
print(df)

import numpy as np

# Your code here
df['Total']= df['Weight (oz.)']*df['Quantity']

#df['Total']= 'Weight (oz.)'*int('Quantity')

print(df)


#def a(Weight):
 
# Total = Weight*Quantity
  
#return Total 
  
df.groupby('Category').agg({'Total': np.sum})


========////
(df.set_index('STNAME').groupby(level=0)['CENSUS2010POP']
    .agg({'avg': np.average, 'sum': np.sum}))

#series groupy above
#dfgroupby below
(df.set_index('STNAME').groupby(level=0)['POPESTIMATE2010','POPESTIMATE2011']
    .agg({'avg': np.average, 'sum': np.sum}))

# NOTE !!!!!!!!!!!!!!
(df.set_index('STNAME').groupby(level=0)['POPESTIMATE2010','POPESTIMATE2011']
    .agg({'POPESTIMATE2010': np.average, 'POPESTIMATE2011': np.sum}))

==>do not pass the col name again,same name in the .agg field as pandas will apply function(np.sum in this case) to the col directly and not create a new hieririchal col
==
#scales 
categor/nominal data
df['Grades'].astype('category').head()
grades = df['Grades'].astype('category',
                             categories=['D', 'D+', 'C-', 'C', 'C+', 'B-', 'B', 'B+', 'A-', 'A', 'A+'],
                             ordered=True)
why we order, cus ordered data has boolean masking true and false, so we can make coparisons suh as
grades > 'C'

===
# reduce ratio/inteval data to categorical data, esp for things like histograms.
the fn cut does that, takes in no of bins which are equally spaced.

df = pd.read_csv('census.csv')
df = df[df['SUMLEV']==50]
df = df.set_index('STNAME').groupby(level=0)['CENSUS2010POP'].agg({'avg': np.average})
pd.cut(df['avg'],10)

# You can also add labels for the sizes [Small < Medium < Large].

pd.cut(s, 3, labels=['Small', 'Medium', 'Large'])
===============

#pivot tables
df.pivot_table(values='(kW)', index='YEAR', columns='Make', aggfunc=np.mean)
==> value is what we are trying to get the mean of..
df.pivot_table(values='(kW)', index='YEAR', columns='Make', aggfunc=[np.mean,np.min], margins=True)
=>margins true, will show overall min/max at the end of the table
->pass any fn to the ..add fn, includ those i define myself

ttest=
# Your code here  -  index='YEAR'

print(Bikes.pivot_table(values='Price', columns=('Bike Type','Manufacturer'), aggfunc=np.mean))


print(Bikes.pivot_table(values='Price', columns=('Manufacturer','Bike Type'), aggfunc=np.mean))

solu=
print(pd.pivot_table(Bikes, index=['Manufacturer','Bike Type']))
===

# Date Functionality in Pandas
>>>>>Timestamp
pd.Timestamp('9/1/2016 10:05AM')


>>>>>>>Period
pd.Period('1/2016')
Period('2016-01', 'M')

pd.Period('3/5/2016')
Period('2016-03-05', 'D')

>>>>>>DatetimeIndex

t1 = pd.Series(list('abc'), [pd.Timestamp('2016-09-01'), pd.Timestamp('2016-09-02'), pd.Timestamp('2016-09-03')])
t1
2016-09-01    a
2016-09-02    b
2016-09-03    c
dtype: object

type(t1.index)
pandas.tseries.index.DatetimeIndex

>>>>>>>PeriodIndex
t2 = pd.Series(list('def'), [pd.Period('2016-09'), pd.Period('2016-10'), pd.Period('2016-11')])
t2
2016-09    d
2016-10    e
2016-11    f
Freq: M, dtype: object
pandas.tseries.period.PeriodIndex

### Converting to Datetime
d1 = ['2 June 2013', 'Aug 29, 2014', '2015-06-26', '7/12/16']
ts3 = pd.DataFrame(np.random.randint(10, 100, (4,2)), index=d1, columns=list('ab'))
ts3

		a	b
2 June 2013	19	39
Aug 29, 2014	54	84
2015-06-26	75	58
7/12/16		46	27

ts3.index = pd.to_datetime(ts3.index)
ts3
==>change data format to a good order
		a	b
2013-06-02	19	39
2014-08-29	54	84
2015-06-26	75	58
2016-07-12	46	27

### Timedeltas - are differences in time

pd.Timestamp('9/3/2016')-pd.Timestamp('9/1/2016')
Timedelta('2 days 00:00:00')

pd.Timestamp('9/2/2016 8:10AM') + pd.Timedelta('12D 3H')
Timestamp('2016-09-14 11:10:00')

###Working with Dates in a Dataframe

dates = pd.date_range('10-01-2016', periods=9, freq='2W-SUN')
dates

DatetimeIndex(['2016-10-02', '2016-10-16', '2016-10-30', '2016-11-13',
               '2016-11-27', '2016-12-11', '2016-12-25', '2017-01-08',
               '2017-01-22'],
              dtype='datetime64[ns]', freq='2W-SUN')

df = pd.DataFrame({'Count 1': 100 + np.random.randint(-5, 10, 9).cumsum(),
                  'Count 2': 120 + np.random.randint(-5, 10, 9)}, index=dates)
df

		Count 1	Count 2
2016-10-02	104	125
2016-10-16	109	122
2016-10-30	111	127
2016-11-13	117	126
2016-11-27	114	126
2016-12-11	109	121
2016-12-25	105	126
2017-01-08	105	125
2017-01-22	101	123


df.index.weekday_name = chk day of the wee date is on
array(['Sunday', 'Sunday', 'Sunday', 'Sunday', 'Sunday', 'Sunday',
       'Sunday', 'Sunday', 'Sunday'], dtype=object)

df.diff() - diff btw each dates value.
df.resample('M').mean() - mean each month in sample

******partial indexing to find info from a particulatr set. e,g yr df['2017'] or month df['2016-12'] or range df['2016-12':]

df.asfreq('W', method='ffill') - change freq of dates in our table to weekly we will get missing vales, so we use forward fill in this code

==>visalize timeseries
import matplotlib.pyplot as plt
%matplotlib inline

df.plot()
================================ss

### Distributions in Pandas

np.random.binomial(1000, 0.5)/1000 - run random binom generator 1 or 0,prob set to .5 - siginifies head or tail
we want to simulate the probability of flipping a fair coin 20 times, and getting a number greater than or equal to 15. Use np.random.binomial(n, p, size) to do 10000 simulations of flipping a fair coin 20 times
print(sum((np.random.binomial(20, 0.5,10000))>15))

chance_of_tornado = 0.01

tornado_events = np.random.binomial(1, chance_of_tornado, 1000000)
    
two_days_in_a_row = 0
for j in range(1,len(tornado_events)-1):
    if tornado_events[j]==1 and tornado_events[j-1]==1:
        two_days_in_a_row+=1

print('{} tornadoes back to back in {} years'.format(two_days_in_a_row, 1000000/365))
===============
distri=
np.random.uniform(0, 1)
np.random.normal(0.75)   or gaussian

distribution = np.random.normal(0.75,size=1000)
np.sqrt(np.sum((np.mean(distribution)-distribution)**2)/len(distribution))
np.std(distribution)

======>shape of tail of distribution   -ve vale means curve is slightly ore flat than normal distrib; +ve meas slightly more peaky than normal distribution

import scipy.stats as stats
stats.kurtosis(distribution)

stats.skew(distribution) - nt much sew as ge with norm distr template/desig

chi_squared_df2 = np.random.chisquare(2, size=10000) #has 1 parameter degrees of freedom,leftskewed

stats.skew(chi_squared_df2)

chi_squared_df5 = np.random.chisquare(5, size=10000)
stats.skew(chi_squared_df5)
#as we increase the degrees of freed, the skew moves to the cetre, we chck plot with a map below:

%matplotlib inline
import matplotlib
import matplotlib.pyplot as plt

output = plt.hist([chi_squared_df2,chi_squared_df5], bins=50, histtype='step', 
                  label=['2 degrees of freedom','5 degrees of freedom'])
plt.legend(loc='upper right')

===
from scipy import stats
stats.ttest_ind?
=> test used for comparig info from a statisti df

stats.ttest_ind(early['assignment1_grade'], late['assignment1_grade'])
Ttest_indResult(statistic=1.400549944897566, pvalue=0.16148283016060577)
=> pvalue higher than set 0.05 so we can not igore as it shows evidce of alternative hypo


=========================
%matplotlib notebook - used to render plots to screens/ jupyter notebookk  - mtp 
import matplotlib as mpl
mpl.get_backend()

# we can pass in '.' to plt.plot to indicate that we want
# the point (3,2) to be indicated with a marker '.'
plt.plot(3, 2, '.')


===
# create a new figure - @as we put new fig, we plot new fig, ignore old one
plt.figure()

# plot the point (3,2) using the circle marker
plt.plot(3, 2, 'o')

# get the current axes
ax = plt.gca()
======================

# get current axes
ax = plt.gca()
# get all the child objects the axes contains
ax.get_children()
==>
[<matplotlib.lines.Line2D at 0x7fb48589bb70>,
 <matplotlib.lines.Line2D at 0x7fb4858eaa20>,
 <matplotlib.lines.Line2D at 0x7fb48589bcf8>,
 <matplotlib.spines.Spine at 0x7fb4858ed080>,
 <matplotlib.spines.Spine at 0x7fb485972198>,
 <matplotlib.spines.Spine at 0x7fb4859e2208>,
 <matplotlib.spines.Spine at 0x7fb48585f390>,
 <matplotlib.axis.XAxis at 0x7fb48585f208>,
 <matplotlib.axis.YAxis at 0x7fb4858cb470>,
 <matplotlib.text.Text at 0x7fb48587bdd8>,
 <matplotlib.text.Text at 0x7fb48587be48>,
 <matplotlib.text.Text at 0x7fb48587beb8>,
 <matplotlib.patches.Rectangle at 0x7fb48587bef0>]


 One, pyplot is going to retrieve the current figure with the function gcf and then get the current axis with the function gca. 
0:42
Pyplot is keeping track of the axis objects for you. But don't forget that they're there and we can get them when we want to get them. 
0:49
Two, also pyplot just mirrors the API of the axis objects. So you can call the plot function against the pyplot module. But this is calling the axis plot functions underneath, so be aware. 
1:03
And three, finally, remember that the function declaration from most of the functions in matplotlib end with an open set of keyword arguments.
=============///////////////////

# Set axis properties [xmin, xmax, ymin, ymax]
ax.axis([0,6,0,10])
=
# Scatterplots
x = np.array([1,2,3,4,5,6,7,8])
y = x

plt.figure()
plt.scatter(x, y) # similar to plt.plot(x, y, '.'), but the underlying child objects in the axes are not Line2D
========
# create a list of colors for each point to have
# ['green', 'green', 'green', 'green', 'green', 'green', 'green', 'red']
colors = ['green']*(len(x)-1)
colors.append('red')

plt.figure()

# plot the point with size 100 and chosen colors
plt.scatter(x, y, s=100, c=colors) #s is size of dots

===========

plt.figure()
# plot a data series 'Tall students' in red using the first two elements of x and y
plt.scatter(x[:2], y[:2], s=100, c='red', label='Tall students')
# plot a second data series 'Short students' in blue using the last three elements of x and y 
plt.scatter(x[2:], y[2:], s=100, c='blue', label='Short students')
# add a label to the x axis
plt.xlabel('The number of times the child kicked a ball')
# add a label to the y axis
plt.ylabel('The grade of the student')
# add a title
plt.title('Relationship between ball kicking and grades')
=====

# Line Plots
import numpy as np

linear_data = np.array([1,2,3,4,5,6,7,8])
exponential_data = linear_data**2

plt.figure()
# plot the linear data and the exponential data
plt.plot(linear_data, '-o', exponential_data, '-o')
==
# fill the area between the linear data and exponential data
plt.gca().fill_between(range(len(linear_data)), 
                       linear_data, exponential_data, 
                       facecolor='blue', 
                       alpha=0.25)
===
# Bar Charts
plt.figure()
xvals = range(len(linear_data))
plt.bar(xvals, linear_data, width = 0.3)


==============
subplot -mtp
plt.figure()
# subplot with 1 row, 2 columns, and current axis is 1st subplot axes
plt.subplot(1, 2, 1)

linear_data = np.array([1,2,3,4,5,6,7,8])

plt.plot(linear_data, '-o')
=
exponential_data = linear_data**2 

# subplot with 1 row, 2 columns, and current axis is 2nd subplot axes
plt.subplot(1, 2, 2)
plt.plot(exponential_data, '-o')
=
# to keep axis the same, use share axis
plt.figure()
ax1 = plt.subplot(1, 2, 1)
plt.plot(linear_data, '-o')
# pass sharey=ax1 to ensure the two subplots share the same y axis
ax2 = plt.subplot(1, 2, 2, sharey=ax1)
plt.plot(exponential_data, '-x')
====
# create a 3x3 grid of subplots
fig, ((ax1,ax2,ax3), (ax4,ax5,ax6), (ax7,ax8,ax9)) = plt.subplots(3, 3, sharex=True, sharey=True)
# plot the linear_data on the 5th subplot axes 
ax5.plot(linear_data, '-')
===
# set inside tick labels to visible
for ax in plt.gcf().get_axes():
    for label in ax.get_xticklabels() + ax.get_yticklabels():
        label.set_visible(True)
==
# necessary on some systems to update the plot
plt.gcf().canvas.draw()
===
#### Histograms
# create 2x2 grid of axis subplots
fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, sharex=True)
axs = [ax1,ax2,ax3,ax4]

# draw n = 10, 100, 1000, and 10000 samples from the normal distribution and plot corresponding histograms
for n in range(0,len(axs)):
    sample_size = 10**(n+1)
    sample = np.random.normal(loc=0.0, scale=1.0, size=sample_size)
    axs[n].hist(sample)
    axs[n].set_title('n={}'.format(sample_size))
===
# repeat with number of bins set to 100
fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, sharex=True)
axs = [ax1,ax2,ax3,ax4]

for n in range(0,len(axs)):
    sample_size = 10**(n+1)
    sample = np.random.normal(loc=0.0, scale=1.0, size=sample_size)
    axs[n].hist(sample, bins=100)
    axs[n].set_title('n={}'.format(sample_size))
==

# use gridspec to partition the figure into subplots
import matplotlib.gridspec as gridspec

plt.figure()
gspec = gridspec.GridSpec(3, 3)

top_histogram = plt.subplot(gspec[0, 1:])
side_histogram = plt.subplot(gspec[1:, 0])
lower_right = plt.subplot(gspec[1:, 1:])
===
Y = np.random.normal(loc=0.0, scale=1.0, size=10000)
X = np.random.random(size=10000)
lower_right.scatter(X, Y)
top_histogram.hist(X, bins=100)
s = side_histogram.hist(Y, bins=100, orientation='horizontal')
===
# clear the histograms and plot normed histograms
top_histogram.clear()
top_histogram.hist(X, bins=100, normed=True)
side_histogram.clear()
side_histogram.hist(Y, bins=100, orientation='horizontal', normed=True)
# flip the side histogram's x axis
side_histogram.invert_xaxis()
==
# change axes limits
for ax in [top_histogram, lower_right]:
    ax.set_xlim(0, 1)
for ax in [side_histogram, lower_right]:
    ax.set_ylim(-5, 5)
==
########### Box and Whisker Plots

import pandas as pd
normal_sample = np.random.normal(loc=0.0, scale=1.0, size=10000)
random_sample = np.random.random(size=10000)
gamma_sample = np.random.gamma(2, size=10000)

df = pd.DataFrame({'normal': normal_sample, 
                   'random': random_sample, 
                   'gamma': gamma_sample})

df.describe()
===
plt.figure()
# create a boxplot of the normal data, assign the output to a variable to supress output
_ = plt.boxplot(df['normal'], whis='range')
# clear the current figure
plt.clf()
# plot boxplots for all three of df's columns
_ = plt.boxplot([ df['normal'], df['random'], df['gamma'] ], whis='range')

===

import mpl_toolkits.axes_grid1.inset_locator as mpl_il

plt.figure()
plt.boxplot([ df['normal'], df['random'], df['gamma'] ], whis='range')
# overlay axis on top of another 
ax2 = mpl_il.inset_axes(plt.gca(), width='60%', height='40%', loc=2)
ax2.hist(df['gamma'], bins=100)
ax2.margins(x=0.5)
===
# switch the y axis ticks for ax2 to the right side
ax2.yaxis.tick_right()
=
# if `whis` argument isn't passed, boxplot defaults to showing 1.5*interquartile (IQR) whiskers with outliers
plt.figure()
_ = plt.boxplot([ df['normal'], df['random'], df['gamma'] ] )
=====
############ Heatmaps
plt.figure()

Y = np.random.normal(loc=0.0, scale=1.0, size=10000)
X = np.random.random(size=10000)
_ = plt.hist2d(X, Y, bins=25)
===
plt.figure()
_ = plt.hist2d(X, Y, bins=100)
# add a colorbar legend
plt.colorbar()
===

######### Animations

import matplotlib.animation as animation

n = 100
x = np.random.randn(n)
===
# create the function that will do the plotting, where curr is the current frame
def update(curr):
    # check if animation is at the last frame, and if so, stop the animation a
    if curr == n: 
        a.event_source.stop()
    plt.cla()
    bins = np.arange(-4, 4, 0.5)
    plt.hist(x[:curr], bins=bins)
    plt.axis([-4,4,0,30])
    plt.gca().set_title('Sampling the Normal Distribution')
    plt.gca().set_ylabel('Frequency')
    plt.gca().set_xlabel('Value')
    plt.annotate('n = {}'.format(curr), [3,27])
===========

fig = plt.figure()
a = animation.FuncAnimation(fig, update, interval=100)
=====
########## Interactivity
plt.figure()
data = np.random.rand(10)
plt.plot(data)

def onclick(event):
    plt.cla()
    plt.plot(data)
    plt.gca().set_title('Event at pixels {},{} \nand data {},{}'.format(event.x, event.y, event.xdata, event.ydata))

# tell mpl_connect we want to pass a 'button_press_event' into onclick when the event is detected
plt.gcf().canvas.mpl_connect('button_press_event', onclick)
==
from random import shuffle
origins = ['China', 'Brazil', 'India', 'USA', 'Canada', 'UK', 'Germany', 'Iraq', 'Chile', 'Mexico']

shuffle(origins)

df = pd.DataFrame({'height': np.random.rand(10),
                   'weight': np.random.rand(10),
                   'origin': origins})
df
=

plt.figure()
# picker=5 means the mouse doesn't have to click directly on an event, but can be up to 5 pixels away
plt.scatter(df['height'], df['weight'], picker=5)
plt.gca().set_ylabel('Weight')
plt.gca().set_xlabel('Height')
=
def onpick(event):
    origin = df.iloc[event.ind[0]]['origin']
    plt.gca().set_title('Selected item came from {}'.format(origin))

# tell mpl_connect we want to pass a 'pick_event' into onpick when the event is detected
plt.gcf().canvas.mpl_connect('pick_event', onpick)
=



def onpick(event):
    origin = df.iloc[event.ind[0]]['origin']
    plt.gca().set_title('Selected item came from {}'.format(origin))

# tell mpl_connect we want to pass a 'pick_event' into onpick when the event is detected
plt.gcf().canvas.mpl_connect('pick_event', onpick)
def onpick(event):
    origin = df.iloc[event.ind[0]]['origin']
    plt.gca().set_title('Selected item came from {}'.format(origin))

# tell mpl_connect we want to pass a 'pick_event' into onpick when the event is detected
plt.gcf().canvas.mpl_connect('pick_event', onpick)

======
t.append(x)
t = t + [x]
t += [x]
And these are wrong:
t.append([x]) # WRONG!
t = t.append(x) # WRONG!
t + [x] # WRONG!
t = t + x # WRONG!
===
reduce: A processing pattern that traverses a sequence and accumulates the elements into
a single result.
map: A processing pattern that traverses a sequence and performs an operation on each
element.
=================
To see whether something appears as a value in a dictionary, you can use the method
values, which returns a collection of values, and then use the in operator:
>>> vals = eng2sp.values()
>>> 'uno' in vals
True
====
A previously computed value that is stored for later use is called a memo #hash // dict
===
To reassign a global variable inside a function you have to declare the global variable before you use it:

been_called = False

def example2():
	global been_called
	been_called = True

The global statement tells the interpreter something like, “In this function, when I say
been_called, I mean the global variable; don’t create a local one.
=================
If a global variable refers to a mutable value, you can modify the value without declaring
the variable:
known = {0:0, 1:1}
def example4():
known[2] = 1
So you can add, remove and replace elements of a global list or dictionary, but if you want
to reassign the variable, you have to declare it:
def example5():
global known
known = dict()
===============
Syntactically, a tuple is a comma-separated list of values:
>>> t = 'a', 'b', 'c', 'd', 'e'
Although it is not necessary, it is common to enclose tuples in parentheses:
>>> t = ('a', 'b', 'c', 'd', 'e')
To create a tuple with a single element, you have to include a final comma:
>>> t1 = 'a',
Another way to create a tuple is the built-in function tuple. With no argument, it creates
an empty tuple:
>>> t = tuple()
tuple assignment
a, b = b, a
addr = 'monty@python.org'
uname, domain = addr.split('@')
Tuples as return values
=
* gathers arguments into a tuple
The complement of gather is scatter. If you have a sequence of values and you want to pass
it to a function as multiple arguments, you can use the * operator
==
zip is a built-in function that takes two or more sequences and returns a list of tuples where
each tuple contains one element from each sequence.
s = 'abcp'
t = [0, 1, 2,2]
p=[4,7,8,0]
print(zip(s, t,p))

for pair in zip(s, t,p):
    print(pair)
zip is not vectorized. so if say 3 elements, in one list, and 4 in another. it will stop at 3
=
You can use tuple assignment in a for loop to traverse a list of tuples:
t = [('a', 0), ('b', 1), ('c', 2)]
for letter, number in t:
print(number, letter)
=
def has_match(t1, t2):
for x, y in zip(t1, t2):
if x == y:
return True
return False

enumerate, gives no to an obj, e.g. 1 a, 2 b etc. can change parameter, index to start say 1 or 2etc.
=
Dictionaries have a method called items that returns a sequence of tuples, where each
tuple is a key-value pair.
=Combining dict with zip yields a concise way to create a dictionary:
>>> d = dict(zip('abc', range(3)))
>>> d
{'a': 0, 'c': 2, 'b': 1}
=
Because tuples are immutable, they don’t provide methods like sort and reverse, which
modify existing lists. But Python provides the built-in function sorted, which takes any
sequence and returns a new list with the same elements in sorted order, and reversed,
which takes a sequence and returns an iterator that traverses the list in reverse order.
=
]]]]]]]]]]]]]]]]]]]]]]Debugging
Compound data structures are useful, but they are prone to what
I call shape errors; that is, errors caused when a data structure has the wrong type, size, or
structure. For example, if you are expecting a list with one integer and I give you a plain
old integer (not in a list), it won’t work.
To help debug these kinds of errors, I have written a module called structshape that
provides a function, also called structshape, that takes any kind of data structure as
an argument and returns a string that summarizes its shape.

===
To choose an element from a sequence at random, you can use choice:
>>> t = [1, 2, 3]
>>> random.choice(t)
===
******************Data structures
thin wat u want to do with data - index, sort hv it permanent state, mutable etc so that u know whic data str to use - tuple, list,string.. tupl of str of lists etc..

===Read write files
To write a file, you have to open it with mode 'w' as a second parameter:
>>> fout = open('output.txt', 'w')
fout = open('output.txt', 'w')
line1 = "This here's the wattle,\n"
fout.write(line1)
fout.close()
=
def ope(d):
    open('{}.txt'.format(d),'w')
==

m='I have spotted %d camels.' % (56)
print(m)
u = 'In %d years I have spotted %g %s.' % (3, 0.1, 'camels')
print(u)
===
database 
The module dbm provides an interface for creating and updating database files
db = dbm.open('captions', 'c')
Some dictionary methods, like keys and items, don’t work with database objects. But
iteration with a for loop works:
for key in db:
print(key, db[key])
As with other files, you should close the database when you are done:
>>> db.close()
=
Pickling
A limitation of dbm is that the keys and values have to be strings or bytes. If you try to use
any other type, you get an error.
The pickle module can help. It translates almost any type of object into a string suitable
for storage in a database, and then translates strings back into objects
pickle.dumps takes an object as a parameter and returns a string representation (dumps is
short for “dump string”):
>>> import pickle
>>> t = [1, 2, 3]
>>> pickle.dumps(t)
b'\x80\x03]q\x00(K\x01K\x02K\x03e.'
The format isn’t obvious to human readers; it is meant to be easy for pickle to interpret.
pickle.loads (“load string”) reconstitutes the object:
>>> t1 = [1, 2, 3]
>>> s = pickle.dumps(t1)
>>> t2 = pickle.loads(s)
>>> t2
[1, 2, 3]

Although the new object has the same value as the old, it is not (in general) the same object:
>>> t1 == t2
True
>>> t1 is t2
False
In other words, pickling and then unpickling has the same effect as copying the object.
You can use pickle to store non-strings in a database. In fact, this combination is so common
that it has been encapsulated in a module called shelve.
==

PIPES
Any program that you can launch from the shell can also be launched from Python using
a pipe object, which represents a running program.
For example, the Unix command ls -l normally displays the contents of the current directory
in long format. You can launch ls with os.popen1
:
>>> cmd = 'ls -l'
>>> fp = os.popen(cmd)
=
The argument is a string that contains a shell command. The return value is an object that
behaves like an open file. You can read the output from the ls process one line at a time
with readline or get the whole thing at once with read:
>>> res = fp.read()
When you are done, you close the pipe like a file:
>>> stat = fp.close()
>>> print(stat)
None
The return value is the final status of the ls process; None means that it ended normally
(with no errors).
========
The only problem with this example is that when you import the module it runs the test
code at the bottom. Normally when you import a module, it defines new functions but it
doesn’t run them.
Programs that will be imported as modules often use the following idiom:
if __name__ == '__main__':
print(linecount('wc.py'))
__name__ is a built-in variable that is set when the program starts. If the program is running
as a script, __name__ has the value '__main__'; in that case, the test code runs. Otherwise,
if the module is being imported, the test code is skipped.
=================================

---------------------Classes and objects----------------------
blank = Point()
print(blank)
blank.x = 3.0
=======
p2 = copy.copy(p1) # for copying objects, ## scratch this -> normal copy() wont work
If you use copy.copy to duplicate a Rectangle, you will find that it copies the Rectangle
object but not the embedded Point.
>>> box2 = copy.copy(box)
>>> box2 is box
False
>>> box2.corner is box.corner
True
Figure 15.3 shows what the object diagram looks like. This operation is called a shallow
copy because it copies the object and any references it contains, but not the embedded
objects.
For most applications, this is not what you want. In this example, invoking
grow_rectangle on one of the Rectangles would not affect the other, but invoking
move_rectangle on either would affect both! This behavior is confusing and error-prone
==============

Fortunately, the copy module provides a method named deepcopy that copies not only the
object but also the objects it refers to, and the objects they refer to, and so on. You will not
be surprised to learn that this operation is called a deep copy.
>>> box3 = copy.deepcopy(box)
>>> box3 is box
False
>>> box3.corner is box.corner
False
box3 and box are completely separate objects.
=
You can also use isinstance to check whether an object is an instance of a class:
>>> isinstance(p, Point)
True
=
If you are not sure whether an object has a particular attribute, you can use the built-in
function hasattr:
>>> hasattr(p, 'x')
True
>>> hasattr(p, 'z')
False
The first argument can be any object; the second argument is a string that contains the name
of the attribute.
You can also use a try statement to see if the object has the attributes you need:
try:
x = p.x
except AttributeError:
x = 0
This approach can make it easier to write functions that work with different types
==============
---------------------Classes and functions----------------------

Pure functions
def add_time(t1, t2):
sum = Time()
sum.hour = t1.hour + t2.hour
sum.minute = t1.minute + t2.minute
sum.second = t1.second + t2.second
return sum
The function creates a new Time object, initializes its attributes, and returns a reference to
the new object. This is called a pure function because it does not modify any of the objects
passed to it as arguments and it has no effect, like displaying a value or getting user input,
other than returning a value.
=
Modifiers
Sometimes it is useful for a function to modify the objects it gets as parameters. In that case,
the changes are visible to the caller. Functions that work this way are called modifiers

write pure functions whenever it is reasonable and resort
to modifiers only if there is a compelling advantage. This approach might be called a
functional programming style.
=
def valid_time(time):
if time.hour < 0 or time.minute < 0 or time.second < 0:
return False
if time.minute >= 60 or time.second >= 60:
return False
return True

=
###########use an assert statement, which checks a given invariant and raises an exception
if it fails:
def add_time(t1, t2):
assert valid_time(t1) and valid_time(t2)
seconds = time_to_int(t1) + time_to_int(t2)
return int_to_time(seconds)
==

---------------------Classes and methods ----------------------
=
class Time:
    def print_time(time):
        print('%.2d:%.2d:%.2d' % (time.hour, time.minute, time.second))
    
    
time2 = Time()
time2.hour = 11
time2.minute = 59
time2.second = 30    
    #print_time(time2)
 
Time.print_time(time2)
time2.print_time()
=
end = start.increment(1337, 460)
TypeError: increment() takes 2 positional arguments but 3 were given
The error message is initially confusing, because there are only two arguments in parentheses.
But the subject is also considered an argument, so all together that’s three
=
a positional argument is an argument that doesn’t have a parameter name;
sketch(parrot, cage, dead=True)
parrot and cage are positional, and dead is a keyword argument
=
The init method
The init method (short for “initialization”) is a special method that gets invoked when an
object is instantiated. Its full name is __init__ (two underscore characters, followed by
init, and then two more underscores). An init method for the Time class might look like
this:
# inside class Time:
def __init__(self, hour=0, minute=0, second=0):
self.hour = hour
self.minute = minute
self.second = second
=
The init method (short for “initialization”) is a special method that gets invoked when an
object is instantiated. 
__str__ is a special method, like __init__, that is supposed to return a string representation
of an object.
============[[[[[[[[[[[[[[[[[[[[****************
if you define a method named __add__ for 
a class, you can use the + operator on the objects
=
t's even possible to overload the "+" operator as well as all the other operators for the purposes of your own class. To do this, you need to understand the underlying mechanism. There is a special (or a "magic") method for every operator sign. The magic method for the "+" sign is the __add__ method. For "-" it is "__sub__" and so on.
=
Operator	Method
+	object.__add__(self, other)
-	object.__sub__(self, other)
*	object.__mul__(self, other)
//	object.__floordiv__(self, other)
/	object.__truediv__(self, other)
%	object.__mod__(self, other)
**	object.__pow__(self, other[, modulo])
<<	object.__lshift__(self, other)
>>	object.__rshift__(self, other)
&	object.__and__(self, other)
^	object.__xor__(self, other)
|	object.__or__(self, other)
==============
Type-based dispatch
=========
In the previous section we added two Time objects, but you also might want to add an
integer to a Time object. The following is a version of __add__ that checks the type of
other and invokes either add_time or increment:
# inside class Time:
def __add__(self, other):
if isinstance(other, Time):
return self.add_time(other)
else:
return self.increment(other)
def add_time(self, other):
seconds = self.time_to_int() + other.time_to_int()
return int_to_time(seconds)
def increment(self, seconds):
seconds += self.time_to_int()
return int_to_time(seconds)
===
Debugging
Another way to access attributes is the built-in function vars, which takes an object and
returns a dictionary that maps from attribute names (as strings) to their values:
>>> p = Point(3, 4)
>>> vars(p)
{'y': 4, 'x': 3}
For purposes of debugging, you might find it useful to keep this function handy:
def print_attributes(obj):***************************
for attr in vars(obj):
print(attr, getattr(obj, attr))
===*********************
basically let eaach def fn in a class be distinct in its operations dnt need diff attrib from other classes. so if  discover a new way to implement a fn, u an redesign w/o having to redesign the entire code.

Interface and implementation****************
One of the goals of object-oriented design is to make software more maintainable, which
means that you can keep the program working when other parts of the system change, and
modify the program to meet new requirements.
A design principle that helps achieve that goal is to keep interfaces separate from implementations.
For objects, that means that the methods a class provides should not depend
on how the attributes are represented.
For example, in this chapter we developed a class that represents a time of day. Methods
provided by this class include time_to_int, is_after, and add_time.
We could implement those methods in several ways. The details of the implementation
depend on how we represent time. In this chapter, the attributes of a Time object are hour,
minute, and second.
As an alternative, we could replace these attributes with a single integer representing the
number of seconds since midnight. This implementation would make some methods, like
is_after, easier to write, but it makes other methods harder.
After you deploy a new class, you might discover a better implementation. If other parts
of the program are using your class, it might be time-consuming and error-prone to change
the interface.
But if you designed the interface carefully, you can change the implementation without
changing the interface, which means that other parts of the program don’t have to change.
*************************
=

Inheritance
=
Variables like suit_names and rank_names, which are defined inside a class but outside
of any method, are called class attributes because they are associated with the class object
Card
=
To
define a new class that inherits from an existing class, you put the name of the existing
class in parentheses:
class Hand(Deck):
=====================
There are several kinds of relationship between classes:
• Objects in one class might contain references to objects in another class. For example,
each Rectangle contains a reference to a Point, and each Deck contains references to
many Cards. This kind of relationship is called HAS-A, as in, “a Rectangle has a
Point.”
• One class might inherit from another. This relationship is called IS-A, as in, “a Hand
is a kind of a Deck.”
• One class might depend on another in the sense that objects in one class take objects
in the second class as parameters, or use objects in the second class as part of a
computation. This kind of relationship is called a dependency
================
def find_defining_class(obj, meth_name):
    for ty in type(obj).mro():
        if meth_name in ty.__dict__:
            return ty

>>> hand = Hand()
>>> find_defining_class(hand, 'shuffle')
<class 'Card.Deck'>
=================
Conditional expressions
y = math.log(x) if x > 0 else float('nan')

def factorial(n):
if n == 0:
return 1
else:
return n * factorial(n-1)

def factorial(n):
return 1 if n == 0 else n * factorial(n-1)

List comprehensions==============
def capitalize_all(t):
res = []
for s in t:
res.append(s.capitalize())
return res
We can write this more concisely using a list comprehension:

def capitalize_all(t):
return [s.capitalize() for s in t]

The bracket operators indicate that we are constructing a new list. The expression inside
the brackets specifies the elements of the list, and the for clause indicates what sequence
we are traversing.
========================Generator expressions
The bracket operators indicate that we are constructing a new list. The expression inside
the brackets specifies the elements of the list, and the for clause indicates what sequence
we are traversing.
==
any and all
any([False, False, True])
True
But it is often used with generator expressions:
>>> any(letter == 't' for letter in 'monty')
True
def avoids(word, forbidden):
return not any(letter in forbidden for letter in word)
The function almost reads like English, “word avoids forbidden if there are not any forbidden
letters in word.
==
Python provides another built-in type, called a set, that behaves like a collection of dictionary
keys with no values. Adding elements to a set is fast; so is checking membership.
And sets provide methods and operators to compute common set operations.
For example, set subtraction is available as a method called difference or as an operator,
-. So we can rewrite subtract like this:
def subtract(d1, d2):
return set(d1) - set(d2)
=
A Counter is like a set, except that if an element appears more than once, the Counter
keeps track of how many times it appears. 
=
defaultdict
==================================
named tuples:*************
lass Point:
def __init__(self, x=0, y=0):
self.x = x
self.y = y
def __str__(self):
return '(%g, %g)' % (self.x, self.y)

from collections import namedtuple
Point = namedtuple('Point', ['x', 'y'])

The first argument is the name of the class you want to create. The second is a list of the
attributes Point objects should have, as strings. The return value from namedtuple is a class
object:
>>> Point
<class '__main__.Point'>
Point automatically provides methods like __init__ and __str__ so you don’t have to
write them
Named tuples provide a quick way to define simple classes. The drawback is that simple
classes don’t always stay simple. You might decide later that you want to add methods
to a named tuple. In that case, you could define a new class that inherits from the named
tuple:
class Pointier(Point):
# add more methods here
==================
Gathering keyword args
def printall(*args):
print(args)
printall(1, 2.0, '3')
(1, 2.0, '3')
=
If you have a dictionary of keywords and values, you can use the scatter operator, ** to
call a function:
>>> d = dict(x=1, y=2)
>>> Point(**d)
Point(x=1, y=2)
Without the scatter operator, the function would treat d as a single positional argument, so
it would assign d to x and complain because there’s nothing to assign to y:
=
hashing######
The total amount of work to run add n
times is proportional to n, so the average time of each add is constant time!






week 4 ===============================
SVMs
And also, it shows the corresponding points in the original input space. So just as we saw with the simple 1D and 2D examples earlier, the kernelized support vector machine tries to find the decision boundary with maximum margin between classes using a linear classifier in the transformed feature space not the original input space. 
9:03
The linear decision boundary learn feature space by linear SVM corresponds to a non-linear decision boundary In the original input space. So in this example, an ellipse like closed region in the input space. Now, one of the mathematically remarkable things about kernelized support vector machines, something referred to as the kernel trick, is that internally, the algorithm doesn't have to perform this actual transformation on the data points to the new high dimensional feature space. 
====
Gamma controls how far the influence of a single trending example reaches, which in turn affects how tightly the decision boundaries end up surrounding points in the input space. 
12:41
Small gamma means a larger similarity radius. So that points farther apart are considered similar. Which results in more points being group together and smoother decision boundaries.
=
## Cross-validation
### Example based on k-NN classifier with fruit dataset (2 features)
from sklearn.model_selection import cross_val_score

clf = KNeighborsClassifier(n_neighbors = 5)
X = X_fruits_2d.as_matrix()
y = y_fruits_2d.as_matrix()
cv_scores = cross_val_score(clf, X, y,cv=10) #cv parameter used to set no of folds

print('Cross-validation scores (3-fold):', cv_scores)
print('Mean cross-validation score (3-fold): {:.3f}'
     .format(np.mean(cv_scores)))
========================
decision tree
 Feature importance is typically a number between 0 and 1 that's assigned to an individual feature. It indicates how important that feature is to the overall prediction accuracy. A feature importance of zero means that the feature is not used at all in the prediction. A feature importance of one, means the feature perfectly predicts the target. Typically, feature importance numbers are always positive and they're normalized so they sum to one.
=
from adspy_shared_utilities import plot_feature_importances

plt.figure(figsize=(10,4), dpi=80)
plot_feature_importances(clf, iris.feature_names)
plt.show()

print('Feature importances: {}'.format(clf.feature_importances_))
# Feature importance is typically a number between 0 and 1 that's assigned to an individual feature. It indicates how important 
#that feature is to the overall prediction accuracy. A feature importance of zero means that the feature is not used at all in 
#the prediction. A feature importance of one, means the feature perfectly predicts the target. Typically, feature importance 
#numbers are always positive and they're normalized so they sum to one.
=================
### Dummy Classifiers
DummyClassifier is a classifier that makes predictions using simple rules, which can be useful as a baseline for comparison against actual classifiers, especially with imbalanced classes.
=
from sklearn.dummy import DummyClassifier

# Negative class (0) is most frequent
dummy_majority = DummyClassifier(strategy = 'most_frequent').fit(X_train, y_train)
# Therefore the dummy 'most_frequent' classifier always predicts class 0
y_dummy_predictions = dummy_majority.predict(X_test)

y_dummy_predictions
=
dummy_majority.score(X_test, y_test)
=
svm = SVC(kernel='linear', C=1).fit(X_train, y_train)
svm.score(X_test, y_test)
=
Use list.sort() when you want to mutate the list, sorted() when you want a new sorted object back.
==================================
=*********************diff lsvm ad linear regression
===============================
Key Differences
Logistic Regression fits the data points as if they are along a continuous function.
This isn't always the case for single-class classification, and so the function may have trouble classifying where P = 0.5
SVM fits a function (hyperplane) that attempts to separate two classes of data that could be of multiple dimensions.
SVM could have difficulty when the classes are not separable or there is not enough margin to fit a (n_dimensions - 1) hyperplane between the two classes.
=
neural networks***************//////////+++++++++++++++++++++++
Solver is the algorithm that actually does the numerical work of finding the optimal weights. And one intuitive way of visualizing this process. Is that all of the solver algorithms have to do a kind of hill-climbing in a very bumpy landscape, with lots of local minima. Where each local minimum corresponds to a locally optimal set of weights. That is, a choice of weight setting that's better than any nearby choices of weights. So across this whole landscape of very bumpy local minima. Some will have higher validation scores on the test data, and some will have lower. So depending on the initial random initialization of the weights. And the nature of the trajectory in the search path that a solver takes through this bumpy landscape. The solver can end up at different local minima, which can have different validation scores. The default solver, adam, tends to be both efficient and effective on large data sets, with thousands of training examples. For small data sets, like many of the ones we use in these examples, the lbfgs solver tends to be faster, and find more effective weights. You can find further details on these more advanced settings in the documentation for scikit-learn. 
==================
===========
===================
At the end of this process, the network attempts to make a prediction as to what’s in the picture. At first, these predictions will appear as random guesses, as no real learning has taken place yet. If the input image is an apple, but “orange” is predicted, the network’s inner layers will need to be adjusted.

The adjustments are carried out through a process called backpropagation to increase the likelihood of predicting “apple” for that same image the next time around. This happens over and over until the predictions are more or less accurate and don’t seem to be improving.
=
The job (which they’ve learned “on the job”) of both the apple and orange nodes is essentially to “vote” for the feature maps that contain their respective fruits. So, the more the “apple” node thinks a particular feature map contains “apple” features, the more votes it sends to that feature map. Both nodes have to vote on every single feature map, regardless of what it contains. So in this case, the “orange” node won’t send many votes to any of the feature maps, because they don’t really contain any “orange” features. In the end, the node that has sent the most votes out — in this example, the “apple” node — can be considered the network’s “answer,” though it’s not quite that simple.
=
=============feature engr
Features that are usable for one data set often are not usable for other data sets (for example the next image data set only contains land animals). The difficulty of feature engineering and the effort involved is the main reason to seek algorithms that can learn features; that is, algorithms that automatically engineer features.
--===

Feature learning can be thought of as Feature Engineering done automatically by algorithms. In deep learning, convolutional layers are exceptionally good at finding good features in images to the next layer to form a hierarchy of nonlinear features that grow in complexity (e.g. blobs, edges -> noses, eyes, cheeks -> faces). The final layer(s) use all these generated features for classification or regression (the last layer in a convolutional net is, essentially, multinomial logistic regression)
===========
activa fn
In contrast, the features of 1000 layers of pure linear transformations can be reproduced by a single layer (because a chain of matrix multiplication can always be represented by a single matrix multiplication). This is why non-linear activation functions are so important in deep learning.
====
More recent convolutional networks use inception modules (see inception) which use 1×1 convolutional kernels to reduce the memory consumption further while speeding up the computation (and thus training)
=
Data leakage==================
You model has learned something, but the thing that it learned is a quirk of the data and not something about the customers that you’re actually trying to model.
=
Customer 11125 has 0 spend on movies and 0 spend on electronics. If the store only sells jewelry, movies, and electronics, then the only way that customer 11125 can possibly be in the dataset is if he bought jewelry. Due to the way that the data was collected and formatted, the condition of having 0 spend on movies and electronics logically implies the condition of having bought jewelry.
=
There’s no such thing as a time machine
The accidental introduction of time-travel into the data is probably the most common source of leakage in machine learning. Due to the way that data is often stored and aggregated, data that are generated at different times may be stored in the same place. A general rule might be: if X happened after Y, you shouldn’t build a model that uses X to predict Y. Claudia Perlich calls this the No Time Machine Condition (NTMC). A model that determines whether it rains at 9:00 AM as a function of whether the road is wet at 9:05 AM might be very accurate, but it will also be very useless.
=
ut this seems like it’s probably the exact opposite of reality! The data collection process results in a negative relationship between the variables DOWNTIME_MINS_2015 and CHURNED, but the real-life process that relates real-life customer churn to real-life downtime minutes could very well have a positive relationship between those two variables! This results in the perverse outcome where the more likely your model says a customer is to churn, the less likely they really are to churn! Your “predictive model” is not only making inaccurate predictions, it is making predictions which are systematically wrong. You’re anti-predicting! Your churn prediction model predicts non-churn! Data leakage has created a model which truly is worse than having no model at all, all the while performing outstandingly well in backtesting.
=
The single most important thing that you can do to prevent data leakage from seeping into your models is to understand the process that generates the data, and how that process relates to the process that you’re actually trying to simulate in order to form predictions. The whole point of modelling is to learn things about some process out there in the real world, and in some sense to approximate that real-world process. But models don’t see the real-world process. They see the data. Leakage happens when there is some systematic correlation between features and outcomes in the data that either does not exist or is not observable in the real world. In order to spot that deceptive correlation, it is crucial to understand the data that you are working with. How is it collected? How is it aggregated? What are the criteria for being included in this sample?
==========================
(i) An "account
number" feature, for the problem of predicting whether a potential
customer would open an account at a bank. Obviously, assignment
of such an account number is only done after an account has been
opened. (ii) An "interviewer name" feature, in a cellular company
churn prediction problem. While the information “who interviewed
the client when they churned” appears innocent enough, it
turns out that a specific salesperson was assigned to take over
cases where customers had already notified they intend to churn
=
no of layers = deep learning algo 
(no of inputs + no of outputs)^0.5 + (1 to 10). to fix the constant value (last part, 0 to 10)
=== iteratively tune the configuration during training using a number of ancillary algorithms
===========

UNSUPERVISED:
------ dimensionality reduction:
With more than two dimensions, the process of finding successive principal components at right angles to the previous ones would continue until the desired number of principal components is reached. One result of applying PCA is that we 
=
There is a family of unsupervised algorithms called Manifold Learning Algorithms that are very good at finding low dimensional structure in high dimensional data and are very useful for visualizations. One classic example of a low dimensional subset in a high dimensional space is this data set in three dimensions, where the points all lie on a two-dimensional sheet with an interesting shape. This lower dimensional sheet within a higher dimensional space is called the manifold. PCA is not sophisticated enough to find this interesting structure. One widely used manifold learning method is called multi-dimensional scaling, or MDS. There are many flavors of MDS, but they all have the same general goal; to visualize a high dimensional dataset and project it onto a lower dimensional space - in most cases, a two-dimensional page - in a way that preserves information about how the points in the original data space are close to each other. In this way, you can find and 
========
An especially powerful manifold learning algorithm for visualizing your data is called t-SNE. t-SNE finds a two-dimensional representation of your data, such that the distances between points in the 2D scatterplot match as closely as possible the distances between the same points in the original high dimensional dataset. In particular, t-SNE gives much more weight to preserving information about distances between points that are neighbors. Here's an example of t-SNE applied to the images in 
t-SNE tends to work better on datasets that have more well-defined local structure; in other words, more clearly defined patterns of neighbors. 
===============
------agglo cluster
One of the nice things about agglomerative clustering is that it automatically arranges the data into a hierarchy as an effect of the algorithm, reflecting the order and cluster distance at which each data point is assigned to successive clusters. 
7:49
This hierarchy can be useful to visualize using what's called a dendrogram, which can be used even with higher dimensional data. 
=======
DBSCAN is an acronym that stands for density-based spatial clustering of applications with noise. 
One advantage of DBSCAN is that you don't need to specify the number of clusters in advance. 

Another advantage is that it works well with datasets that have more complex cluster shapes. 
It can also find points that are outliers that shouldn't reasonably be assigned to any cluster. DBSCAN is relatively efficient and can be used for large datasets. The main idea behind DBSCAN is that clusters represent areas in the dataspace that are more dense with data points, while being separated by regions that are empty or at least much less densely populate
===================
The two main parameters for DBSCAN are min samples and eps. 
All points that lie in a more dense region are called core samples. 
For a given data point, if there are min sample of other data points that lie within a distance of eps, that given data points is labeled as a core sample. 

Then, all core samples that are with a distance of eps units apart are put into the same cluster. 
In addition to points being categorized as core samples, points that don't end up belonging to any cluster are considered as noise. While points that are within a distance of eps units from core points, but not core points themselves, are termed boundary points. 
===============================
K-means clustering
Use when:
…you have an idea of how many groups you’re expecting to find a priori.
=
Hierarchical clustering
Use when:
…you wish to uncover the underlying relationships between your observations.
=
Graph Community Detection
Use when:
…you have data that can be represented as a network, or ‘graph’.
=***********
Worked example:
Graph theory, or the mathematical study of networks, is a fascinating branch of mathematics that lets us model complex systems as an abstract collection of ‘dots’ (or vertices) connected by ‘lines’ (or edges).

Perhaps the most intuitive case-studies are social networks. Here, the vertices represent people, and edges connect vertices who are friends/followers. However, any system can be modelled as a network if you can justify a method to meaningfully connect different components. Among the more innovative applications of graph theory to clustering include feature extraction from image data, and analysing gene regulatory networks.
=================
.       - Any Character Except New Line
\d      - Digit (0-9)
\D      - Not a Digit (0-9)
\w      - Word Character (a-z, A-Z, 0-9, _)
\W      - Not a Word Character
\s      - Whitespace (space, tab, newline)
\S      - Not Whitespace (space, tab, newline)

\b      - Word Boundary
\B      - Not a Word Boundary
^       - Beginning of a String
$       - End of a String

[]      - Matches Characters in brackets
[^ ]    - Matches Characters NOT in brackets
|       - Either Or
( )     - Group

Quantifiers:
*       - 0 or More
+       - 1 or More
?       - 0 or One
{3}     - Exact Number
{3,4}   - Range of Numbers (Minimum, Maximum)


#### Sample Regexs ####

[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\.[a-zA-Z0-9-.]+
==================
And you need better models, and you can learn those using supervised learning, 
nlt nlp text parsing
==

text classification:
The bag-of-words approach is simple and commonly used way to represent text for use in machine learning, which ignores structure and only counts how often each word occurs. CountVectorizer allows us to use the bag-of-words approach by converting a collection of text documents into a matrix of token counts. 
========================
Fitting the CountVectorizer consists of the tokenization of the trained data and building of the vocabulary. 

Fitting the CountVectorizer tokenizes each document by finding all sequences of characters of at least two letters or numbers separated by word boundaries. Converts everything to lowercase and builds a vocabulary using these tokens. 
==============
High weight is given to terms that appear often in a particular document, but don't appear often in the corpus. Features with low tf–idf are either commonly used across all documents or rarely used and only occur in long documents. 

Features with high tf–idf are frequently used within specific documents, but rarely used across all documents. 

====
One way we can add some context is by adding sequences of word features known as n-grams. For example, bigrams, which count pairs of adjacent words, could give us features such as is working versus not working. And trigrams, which give us triplets of adjacent words, could give us features such as not an issue. 

To create these n-gram features, we'll pass in a tuple to the parameter ngram_range, where the values correspond to the minimum length and maximum lengths of sequences. 

For example, if I pass in the tuple, 1, 2, CountVectorizer will create features using the individual words, as well as the bigrams. 
========================

# just how much data did we lose?
print("Columns in original dataset: %d \n" % nfl_data.shape[1])
print("Columns with na's dropped: %d" % columns_with_na_dropped.shape[1])

=++++
Columns in original dataset: 102 

Columns with na's dropped: 41

axis = 0 applies the method row-by-row.

axis = 1 applies the method column-by-column.
==================
*************//////////////-----------+++++++++++
Scaling
 you'll only want to normalize your data if you're going to be using a machine learning or statistics technique that assumes your data is normally distributed. Some examples of these include t-tests, ANOVAs, linear regression, linear discriminant analysis (LDA) and Gaussian naive Bayes. (Pro tip: any method with "Gaussian" in the name probably assumes normality.)
**************-------------------------+++++++++++++++++
## Normalization

Scaling just changes the range of your data. Normalization is a more radical transformation. The point of normalization is to change your observations so that they can be described as a normal distribution.

The method were  using to normalize here is called the [Box-Cox Transformation](https://en.wikipedia.org/wiki/Power_transform#Box%E2%80%93Cox_transformation). 
===============
==============
ValueError: x and y must be the same size: X_train is 2d (matrix with a single column), while y_train 1d (vector). In turn you get different sizes

using X_train[:,0] for plotting

OR

Slicing with [:, :-1] will give you a 2-dimensional array (including all rows and all columns excluding the last column).

Slicing with [:, 1] will give you a 1-dimensional array (including all rows from the second column). To make this array also 2-dimensional use [:, 1:2] or [:, 1].reshape(-1, 1) or [:, 1][:, None] instead of [:, 1]. This will make x and y comparable.
========================
==================
================

DEEP LEARNING
=
For example, in the real estate application that we saw in the previous video, we use a universally standard neural network architecture, right? Maybe for real estate and online advertising might be a relatively standard neural network, like the one that we saw. 
For image applications we'll often use convolution on neural networks, often abbreviated CNN. 
And for sequence data. So for example, audio has a temporal component, right? Audio is played out over time, so audio is most naturally represented as a one-dimensional time series or as a one-dimensional temporal sequence. And so for sequence data, you often use an RNN, a recurrent neural network. Language, English and Chinese, the alphabets or the words come one at a time. So language is also most naturally represented as sequence data. And so more complex versions of RNNs are often used for these applications. And then, for more complex applications, like autonomous driving, where you have an image, that might suggest more of a CNN convolution neural network structure and radar info which is something quite different. You might end up with a more custom, or some more complex, hybrid neural network architecture. 
=
http://bit.ly/2uGREG3


===

RL 1 - ML REINFORCE
POLICY - a fn that tells you wat action you should take at any stage you come across
markov is all about - what action  should take at a stage; not the seqece of actions aka a plan, of things you ought to follow
tells u wat to do everywhere as against a plan and is robust to stochasticity of the world.
stochastic is an adjective in English that describes something that was randomly determined

=====
javascriptp

DOM Selectors
--------------
getElementsByTagName
getElementsByClassName
getElementById

querySelector
querySelectorAll

getAttribute
setAttribute

##Changing Styles
style.{property} //ok

className //best
classList //best

classList.add
classList.remove
classList.toggle

##Bonus
innerHTML //DANGEROUS

parentElement
children

##It is important to CACHE selectors in variables
==================
.loc for label based indexing or
.iloc for positional indexing
==========
>>> X = [[0., 0.], [1., 1.]]
>>> y = [[0, 1], [1, 1]]
>>> clf = MLPClassifier(solver='lbfgs', alpha=1e-5,
...                     hidden_layer_sizes=(15,), random_state=1)
...
>>> clf.fit(X, y)                         
MLPClassifier(activation='relu', alpha=1e-05, batch_size='auto',
       beta_1=0.9, beta_2=0.999, early_stopping=False,
       epsilon=1e-08, hidden_layer_sizes=(15,), learning_rate='constant',
       learning_rate_init=0.001, max_iter=200, momentum=0.9,
       nesterovs_momentum=True, power_t=0.5, random_state=1, shuffle=True,
       solver='lbfgs', tol=0.0001, validation_fraction=0.1, verbose=False,
       warm_start=False)


==========
https://edwisor.com/?ref=Quorap

 

https://imarticus.org/certification-in-sas/?id=Website_Organic_Quora

 

https://www.sas.com/en_in/training/academy-data-science.html

 

https://www.sas.com/en_in/training/academy-data-science.html

 

https://www.quora.com/Is-SAS-certification-useful

 

https://www.sas.com/en_my/training/academy-data-science.html
https://www.cio.com/article/3209911/certifications/big-data-certifications-that-will-pay-off.html

predict_prob() to get classes with RF, use soft.

Baggi,nn,swm,pocle pca,use pca seeting to account for max var ratio eg 95%

https://medium.com/implodinggradients/tensorflow-or-keras-which-one-should-i-learn-5dd7fa3f9ca0

https://www.quora.com/What-is-the-future-of-TensorFlow
===
CLOUD AUTOML google
http://simplify.ai/
https://indico.io/
https://lobe.ai/
==
ef9a74eb187f473696dcf4ef67c43a71
===
 onRouteChange = ()=>{
    if (route === 'SignIn'){
    this.setState({route: 'SignIn3'}
  )
  else{this.setState({route: 'SignIn'})


====


  { /*onRouteChange = ()=>{
    this.setState({route: 'SignIn3'})
  };*/ }  

===

{ this.state.route === 'SignIn'
===

if (this.state.route === 'SignInOut'){
    this.setState({route: 'SignIn'}
  )}
  else{this.setState({route: 'SignInOut'})
==
<Navigation isSignedIn ={this.state.isSignedIn}
==
stupid thing - waSted my time::::::;;

onRouteChange = (route)=>{
  if (route === 'SignIn'){
    this.setState({isSignedIn:false})} else if (route === 'home'){
      this.setState({isSignedIn: true})

    }
========

express js
const user= {
	name: 'Sally',
	hobby: 'piano'
}
=========
https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQaO0ebhM8DlKWbHffXpob7x-kYMP9qV-XIbBaM_u9Kf_Gwsrei9A


====
deleted windows path to make space for more
C:\Program Files (x86)\Microsoft SQL Server\120\DTS\Binn\

PATH=%PATH%;C:\Program Files\PostgreSQL\9.2\bin


===
$npm install --save bcrypt-nodejs && npm uninstall --save bcrypt
 env|grep PATH
set PATH=%PATH%;C:\Program Files\PostgreSQL\10\bin\
PATH=%PATH%;C:\Program Files\PostgreSQL\10\bin

set DATABASE_URL=postgres://localhost/databasename

postgres -D /usr/local/pgsql/data



https://enigmatic-taiga-56030.herokuapp.com/
#BigData #Algorithms #DataUsage #NationalPolicy
===========
kim@yahoo
123s

bb
bb
=========
https://ae.linkedin.com/in/kirill-smolyakov-5751b8145
========

CREATE TABLE public.login
(
    id integer NOT NULL DEFAULT nextval('login_id_seq'::regclass),
    hash character varying(200) COLLATE pg_catalog."default" NOT NULL,
    email text COLLATE pg_catalog."default" NOT NULL,
    CONSTRAINT login_pkey PRIMARY KEY (id),
    CONSTRAINT login_email_key UNIQUE (email)
============
CREATE TABLE public.users
(
    id integer NOT NULL DEFAULT nextval('users_id_seq'::regclass),
    name character varying(200) COLLATE pg_catalog."default",
    email text COLLATE pg_catalog."default" NOT NULL,
    entries bigint DEFAULT 0,
    joined timestamp without time zone NOT NULL,
    CONSTRAINT users_pkey PRIMARY KEY (id),
    CONSTRAINT users_email_key UNIQUE (email)
)
=======


p - jerg - 123
=====
SQL

CREATE TABLE login( 
id serial PRIMARY KEY,
hash varchar(100) NOT NULL,
email text UNIQUE NOT NULL
);

=

create table  users( id serial PRIMARY KEY,
name varchar(100),
email text UNIQUE NOT NULL,
entries BIGINT DEFAULT 0,
joined TIMESTAMP NOT NULL
);

==
CREATE TABLE login(id serial PRIMARY KEY,hash varchar(100) NOT NULL,email text UNIQUE NOT NULL);

========

c.NotebookApp.keyfile = r'C:\Users\Dmob\certs\mycert.pem' # path to the certificate key we generated
=s
===