python code

To create an environment, use conda create -n env_name list of packages in your terminal. Here -n env_name sets the name of your environment (-n for name) and list of packages is the list of packages you want installed in the environment. For example, to create an environment named my_env and install numpy in it, type conda create -n my_env numpy

ou can specify which version of Python to install in the environment. This is useful when you're working with code in both Python 2.x and Python 3.x. To create an environment with a specific Python version, do something like conda create -n py3 python=3 or conda create -n py2 python=2

Entering an environment
On Windows, use activate my_env

check this out with conda list. Installing packages in the environment is the same as before: conda install package_name. Only this time, the specific packages you install will only be available when you're in the environment. To leave the environment, type source deactivate (on OSX/Linux). On Windows, use deactivate

Saving and loading environments
A really useful feature is sharing environments so others can install all the packages used in your code, with the correct versions. You can save the packages to a YAML file with conda env export > environment.yaml. The first part conda env export writes out all the packages in the environment, including the Python version.

Above you can see the name of the environment and all the dependencies (along with versions) are listed. The second part of the export command, > environment.yaml writes the exported text to a YAML file environment.yaml. This file can now be shared and others will be able to create the same environment you used for the project.

To create an environment from an environment file use conda env create -f environment.yaml. This will create a new environment with the same name listed in environment.yaml.

Listing environments
If you forget what your environments are named (happens to me sometimes), use conda env list to list out all the environments you've created. You should see a list of environments, there will be an asterisk next to the environment you're currently in. The default environment, the environment used when you aren't in one, is called root.

Removing environments
If there are environments you don't use anymore, conda env remove -n env_name will remove the specified environment (here, named env_name).

=========
Best practices
Using environments
One thing that’s helped me tremendously is having separate environments for Python 2 and Python 3. I used conda create -n py2 python=2 and conda create -n py3 python=3 to create two separate environments, py2 and py3. Now I have a general use environment for each Python version. In each of those environments, I've installed most of the standard data science packages (numpy, scipy, pandas, etc.). Remember that when you set up an environment initially, you'll only start with the standard packages and whatever packages you specify in your conda create statement.

I’ve also found it useful to create environments for each project I’m working on. It works great for non-data related projects too like web apps with Flask. For example, I have an environment for my personal blog using Pelican.

Sharing environments
When sharing your code on GitHub, it's good practice to make an environment file and include it in the repository. This will make it easier for people to install all the dependencies for your code. I also usually include a pip requirements.txt file using pip freeze (learn more here) for people not using conda.
============
Use + to concatenate lists.
Use `*` to repeat lists.
Use the in operator to check if something is inside a list.


==
regressio analysis

y = melbourne_data.Price
melbourne_predictors = ['Rooms', 'Bathroom', 'Landsize', 'BuildingArea', 'YearBuilt', 'Lattitude', 'Longtitude']
X = melbourne_data[melbourne_predictors]
# Define model
melbourne_model = DecisionTreeRegressor()

# Fit model
melbourne_model.fit(X, y)
print("Making predictions for the following 5 houses:")
print(X.head())
print("The predictions are")
print(melbourne_model.predict(X.head()))
==
from sklearn.metrics import mean_absolute_error
from sklearn.model_selection import train_test_split

# split data into training and validation data, for both predictors and target
# The split is based on a random number generator. Supplying a numeric value to
# the random_state argument guarantees we get the same split every time we
# run this script.

train_X, val_X, train_y, val_y = train_test_split(X, y,random_state = 0)
# Define model
melbourne_model = DecisionTreeRegressor()
# Fit model
melbourne_model.fit(train_X, train_y)

# get predicted prices on validation data
val_predictions = melbourne_model.predict(val_X)
print(mean_absolute_error(val_y, val_predictions))
MAE - On average, our predictions are off by about X

from sklearn.metrics import mean_absolute_error
from sklearn.tree import DecisionTreeRegressor

def get_mae(max_leaf_nodes, predictors_train, predictors_val, targ_train, targ_val):
    model = DecisionTreeRegressor(max_leaf_nodes=max_leaf_nodes, random_state=0)
    model.fit(predictors_train, targ_train)
    preds_val = model.predict(predictors_val)
    mae = mean_absolute_error(targ_val, preds_val)
    return(mae)
=
Random Forests
forest_model = RandomForestRegressor()
forest_model.fit(train_X, train_y)
melb_preds = forest_model.predict(val_X)
print(mean_absolute_error(val_y, melb_preds))

TEST DATA
import numpy as np
import pandas as pd
from sklearn.ensemble import RandomForestRegressor

# Read the data
train = pd.read_csv('../input/train.csv')

# pull data into target (y) and predictors (X)
train_y = train.SalePrice
predictor_cols = ['LotArea', 'OverallQual', 'YearBuilt', 'TotRmsAbvGrd']

# Create training predictors data
train_X = train[predictor_cols]

my_model = RandomForestRegressor()
my_model.fit(train_X, train_y)

# Read the test data
test = pd.read_csv('../input/test.csv')
# Treat the test data in the same way as training data. In this case, pull same columns.
test_X = test[predictor_cols]
# Use the model to make predictions
predicted_prices = my_model.predict(test_X)
# We will look at the predicted prices to ensure we have something sensible.
print(predicted_prices)

===
Prepare Submission File

We make submissions in CSV files. Your submissions usually have two columns: an ID column and a prediction column. The ID field comes from the test data (keeping whatever name the ID field had in that data, which for the housing data is the string 'Id'). The prediction column will use the name of the target field.

We will create a DataFrame with this data, and then use the dataframe's to_csv method to write our submission file. Explicitly include the argument index=False to prevent pandas from adding another column in our csv file

my_submission = pd.DataFrame({'Id': test.Id, 'SalePrice': predicted_prices})
# you could use any filename. We choose submission here
my_submission.to_csv('submission.csv', index=False)
===

1) A Simple Option: Drop Columns with Missing Values¶

ou'll have both a training dataset and a test dataset. You will want to drop the same columns in both DataFrames. In that case, you would write

cols_with_missing = [col for col in original_data.columns 
                                 if original_data[col].isnull().any()]
redued_original_data = original_data.drop(cols_with_missing, axis=1)
reduced_test_data = test_data.drop(cols_with_missing, axis=1)

=
2) A Better Option: Imputation
Imputation fills in the missing value with some number. The imputed value won't be exactly right in most cases, but it usually gives more accurate models than dropping the column entirely.

This is done with

from sklearn.preprocessing import Imputer
my_imputer = Imputer()
data_with_imputed_values = my_imputer.fit_transform(original_data)
=
# make copy to avoid changing original data (when Imputing)
new_data = original_data.copy()

# make new columns indicating what will be imputed
cols_with_missing = (col for col in new_data.columns 
                                 if new_data[c].isnull().any())
for col in cols_with_missing:
    new_data[col + '_was_missing'] = new_data[col].isnull()

# Imputation
my_imputer = Imputer()
new_data = my_imputer.fit_transform(new_data)
\\
df.isnull().any() generates a boolean array (True if the column has a missing value, False otherwise). You can use it to index into df.columns:

df.columns[df.isnull().any()]
will return a list of the columns which have missing values.
=

Basic Problem Set-up
In [1]:
import pandas as pd

# Load data
melb_data = pd.read_csv('../input/melbourne-housing-snapshot/melb_data.csv')

from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_error
from sklearn.model_selection import train_test_split

melb_target = melb_data.Price
melb_predictors = melb_data.drop(['Price'], axis=1)

# For the sake of keeping the example simple, we'll use only numeric predictors. 
melb_numeric_predictors = melb_predictors.select_dtypes(exclude=['object'])
=
y = melb_target
X = melb_data[melb_numeric_predictors2]
print(7*8)
#print(melb_numeric_predictors.columns)

X_train, X_test, y_train, y_test = train_test_split(X, y,random_state = 0) 
#score_dataset(X_train, X_test, y_train, y_test) 

=====

Best Prepare Your Data For Naive Bayes
Categorical Inputs: Naive Bayes assumes label attributes such as binary, categorical or nominal.
Gaussian Inputs: If the input variables are real-valued, a Gaussian distribution is assumed. In which case the algorithm will perform better if the univariate distributions of your data are Gaussian or near-Gaussian. This may require removing outliers (e.g. values that are more than 3 or 4 standard deviations from the mean).
Classification Problems: Naive Bayes is a classification algorithm suitable for binary and multiclass classification.
Log Probabilities: The calculation of the likelihood of different class values involves multiplying a lot of small numbers together. This can lead to an underflow of numerical precision. As such it is good practice to use a log transform of the probabilities to avoid this underflow.
Kernel Functions: Rather than assuming a Gaussian distribution for numerical input values, more complex distributions can be used such as a variety of kernel density functions.
Update Probabilities: When new data becomes available, you can simply update the probabilities of your model. This can be helpful if the data changes frequently.

Advantages and Disadvantage of Naive Bayes classifier
Advantages
Naive Bayes Algorithm is a fast, highly scalable algorithm.
Naive Bayes can be use for Binary and Multiclass classification. It provides different types of Naive Bayes Algorithms like GaussianNB, MultinomialNB, BernoulliNB.
It is a simple algorithm that depends on doing a bunch of counts.
Great choice for Text Classification problems. It’s a popular choice for spam email classification.
It can be easily train on small dataset
Disadvantages
It considers all the features to be unrelated, so it cannot learn the relationship between features. E.g., Let’s say Remo is going to a part. While cloth selection for the party, Remo is looking at his cupboard. Remo likes to wear a white color shirt. In Jeans, he likes to wear a brown Jeans, But Remo doesn’t like wearing a white shirt with Brown Jeans. Naive Bayes can learn individual features importance but can’t determine the relationship among features.

This stands in contrast to the instinctive way humans often behave, where, having concluded that they should not believe in Bookcase Aliens on the basis of the evidence in front of them, they discard that evidence entirely, denounce it, and say that it was never any evidence at all. (This is "treating arguments like soldiers" and acting like any evidence in favor of a proposition has to be "defeated.")

The Bayesian just says "yes, that is evidence in favor of the claim, but it's not quantitatively enough evidence." This idiom also stands in contrast to the practice of treating any concession an opponent makes as a victory. If true claims are supposed to have all their arguments upheld and false claims are supposed to have all their enemy arguments defeated, then a single undefeated claim of support stands as a proof of victory, no matter how strong or weak the evidence that it provides. Not so with Bayesians — a Bayesian considers the bookcase observation to be locally a piece of evidence favoring Bookcase Aliens, just massively insufficient evidence.
///////////
The Weber-Fechner law says that most human sensory perceptions are logarithmic, in the sense that a factor-of-2 intensity change feels like around the same amount of increase no matter where you are on the scale. Doubling the physical intensity of a sound feels to a human like around the same amount of change in that sound whether the initial sound was 40 decibels or 60 decibels. That's why there's an exponential decibel scale of sound intensities in the first place!

Thus the log-odds form should be, in a certain sense, the most intuitive variant of Bayes' rule to use: Just add the evidence-strength to the belief-strength! If you can make your feelings of evidence-strength and belief-strength be proportional to the logarithms of ratios, that is.

Finally, the log-odds representation gives us an even easier way to see how extraordinary claims require extraordinary evidence: If your prior belief in H is -30 bits, and you see evidence on the order of +5 bits for H, then you're going to wind up with -25 bits of belief in H, which means you still think it's far less likely than the alternatives.
///////////////
In the case of discrete inputs (indicator or frequency features for discrete events), naive Bayes classifiers form a generative-discriminative pair with (multinomial) logistic regression classifiers: each naive Bayes classifier can be considered a way of fitting a probability model that optimizes the joint likelihood {\displaystyle p(C,\mathbf {x} )} p(C,{\mathbf  {x}}), while logistic regression fits the same probability model to optimize the conditional {\displaystyle p(C\mid \mathbf {x} )} {\displaystyle p(C\mid \mathbf {x} )}
==============

you'll need str(), to convert a value into a string. str(savings), for example, will convert the float savings to a string.

Similar functions such as int(), float() and bool() will help you convert Python values into any type.

===

11. Representation Does Not Imply Learnable
Just because a function can be represented, does not mean that the function can actually be learnt. Restrictions imposed by data, time and memory, limit the functions that can actually be learnt in a feasible manner. For example, decision tree learners can not learn trees with more leaves than the number of training data points. The right question to ask is "whether a function can be learnt" and not "whether a function can be represented".

1. Learning = Representation + Evaluation + Optimization
All machine learning algorithms have three components:

Representation for a learner is the set if classifiers/functions that can be possibly learnt. This set is called hypothesis space. If a function is not in hypothesis space, it can not be learnt.
Evaluation function tells how good the machine learning model is.
Optimisation is the method to search for the most optimal learning model.
=====
matplotlib

# Change the line plot below to a scatter plot

plt.scatter(gdp_cap, life_exp)


# Put the x-axis on a logarithmic scale

plt.xscale('log')

=========*****************
sales_record = {
'price': 3.24,
'num_items': 4,
'person': 'Chris'}

sales_statement = '{} bought {} item(s) at a price of {} each for a total of {}'

print(sales_statement.format(sales_record['person'],
                             sales_record['num_items'],
                             sales_record['price'],
                             sales_record['num_items']*sales_record['price']))
*******************++++++++++++++++++++++++

# Reading and Writing CSV files
import csv

%precision 2

with open('mpg.csv') as csvfile:
    mpg = list(csv.DictReader(csvfile))
    
mpg[:3] # The first three dictionaries in our list.
==

create set to return the unique values for the number of cylinders the cars in our dataset
cylinders = set(d['cyl'] for d in mpg)
cylinders

=
more complex example where we are grouping the cars by number of cylinder, and finding the average cty mpg for each group


CtyMpgByCyl = []

for c in cylinders: # iterate over all the cylinder levels
    summpg = 0
    cyltypecount = 0
    for d in mpg: # iterate over all dictionaries
        if d['cyl'] == c: # if the cylinder level type matches,
            summpg += float(d['cty']) # add the cty mpg
            cyltypecount += 1 # increment the count
    CtyMpgByCyl.append((c, summpg / cyltypecount)) # append the tuple ('cylinder', 'avg mpg')

CtyMpgByCyl.sort(key=lambda x: x[0])
CtyMpgByCyl

=======*************
timedelta is a duration expressing the difference between two dates.


delta = dt.timedelta(days = 100) # create a timedelta of 100 days
delta
==============
# The Python Programming Language: Objects and map()
class Person:
    department = 'School of Information' #a class variable

    def set_name(self, new_name): #a method
        self.name = new_name
    def set_location(self, new_location):
        self.location = new_location
==============

class Person:
    department = 'School of Information' #a class variable

    def set_name(self, new_name): #a method 
        self.name = new_name
    def set_location(self, new_location):
        self.location = new_location
        
#To define a method, you just write it as you would have a function. The one change, is that to have access to the instance which a method is being invoked upon, you must include self, in the method signature. 
#Similarly, if you want to refer to instance variables set on the object, you prepend them with the word self, with a full stop.
#no priv or protec class #javarocks
#no need constrctor u can add __init__, still not necessary, call oject w/ person(); then add '.' notation to call its methods
=================

map() fn

But when we go to print out the map, we see that we get an odd reference value instead of a list of items that we're expecting. This is called lazy evaluation. In Python, the map function returns to you a map object. It doesn't actually try and run the function min on two items, until you look inside for a value. This is an interesting design pattern of the language, and it's commonly used when dealing with big data. This allows us to have very efficient memory management, even though something might be computationally complex. 
5:30
Maps are iterable, just like lists and tuples, so we can use a for loop to look at all of the values in the map. 
====

### The Python Programming Language: Lambda and List Comprehensions #anon fn

my_function = lambda a, b, c : a + b

my_function(1, 2, 3)

##################
You declare a lambda function with the word lambda followed by a list of arguments, followed by a colon and then a single expression and this is key. There's only one expression to be evaluated in a lambda. The expression value is returned on execution of the lambda. 
0:55
The return of a lambda is a function reference. So in this case, you would execute my_function and pass in three different parameters. 
##############

my_list = []
for number in range(0, 1000):
    if number % 2 == 0:
        my_list.append(number)
my_list

quick list creation===========
my_list = [number for number in range(0,1000) if number % 2 == 0]
my_list
=====
lowercase = 'abcdefghijklmnopqrstuvwxyz'
digits = '0123456789'

correct_answer = [a+b+c+d for a in lowercase for b in lowercase for c in digits for d in digits]

====
n = np.arange(0, 30, 2) # start at 0 count up by 2, stop before 30
n = n.reshape(3, 5) # reshape array to be 3x5
o = np.linspace(0, 4, 9) # return 9 evenly spaced values from 0 to 4
resize changes the shape and size of array in-place.
o.resize(3, 3)
np.eye(3);ones,zeros
np.diag(y)
=
Create an array using repeating list (or see np.tile)
np.array([1, 2, 3] * 3)
=np.repeat([1, 2, 3], 3)
=array([1, 1, 1, 2, 2, 2, 3, 3, 3])
=
Dot Product:x.dot(y) # dot product  1*4 + 2*5 + 3*6
=
argmax` and `argmin` return the index of the maximum and minimum values in the array.
=====================

for i, j in zip(test, test2):
    print(i,'+',j,'=',i+j)

========
Querying a Series pandas
s.iloc[2] - query by index
s.lo['golf'] - query by manually stored created index
s.loc[5]=45; adds to series s, value 4, with index 5


# Dataframe Indexing and Loading
df['Location'] = None  - addds extra column to dataframe
costs+=2   -iterate,add on a col

!cat olympics.csv - shell commad,read csv
=

df = pd.read_csv('olympics.csv', index_col = 0, skiprows=1)
index_col = 0; use first col as index; skiprows =1, use 1st row as column headers

# Querying a DataFrame
only_gold = df.where(df['Gold'] > 0)
df[(df['Gold.1'] > 0) & (df['Gold'] == 0)]

# Indexing Dataframes
df['country'] = df.index #create new col country
df = df.set_index('Gold') #set gold as index, - this call destroys the preset index gold

df = df.set_index(['STNAME', 'CTYNAME'])
df.head()

#see 2 elements/rows of a multi-labeled index df, series pandas.. -hierarchically index 
df.loc[ [('Michigan', 'Washtenaw County'),
         ('Michigan', 'Wayne County')] ]


#test
df = df.set_index([df.index, 'Name'])
df.index.names = ['Location', 'Name']
df = df.append(pd.Series(data={'Cost': 3.00, 'Item Purchased': 'Kitty Food'}, name=('Store 2', 'Kevyn')))
df

# Merging Dataframes
staff_df = pd.DataFrame([{'Name': 'Kelly', 'Role': 'Director of HR'},
                         {'Name': 'Sally', 'Role': 'Course liasion'},
                         {'Name': 'James', 'Role': 'Grader'}])
staff_df = staff_df.set_index('Name')
student_df = pd.DataFrame([{'Name': 'James', 'School': 'Business'},
                           {'Name': 'Mike', 'School': 'Law'},
                           {'Name': 'Sally', 'School': 'Engineering'}])
student_df = student_df.set_index('Name')

pd.merge(staff_df, student_df, how='outer', left_index=True, right_index=True)
how - is for merge type, left,right index for index to e used for joininh, index can be col name as below.
=
products = products.reset_index()

invoices=invoices.reset_index()
###reset index before merging dataframes
pd.merge(staff_df, student_df, how='left', left_on='Name', right_on='Name') - you can also use indices to join them


#Idiomatic Pandas: Making Code Pandorable


======
#applying a fn across a df,use applymap across a row, apply.
import numpy as np
def min_max(row):
    data = row[['POPESTIMATE2010',
                'POPESTIMATE2011',
                'POPESTIMATE2012',
                'POPESTIMATE2013',
                'POPESTIMATE2014',
                'POPESTIMATE2015']]
    return pd.Series({'min': np.min(data), 'max': np.max(data)})
#or - row['max'] = np.max(data)
    row['min'] = np.min(data)
    return row


df.apply(min_max, axis=1)
====
lambda 

rows = ['POPESTIMATE2010',
        'POPESTIMATE2011',
        'POPESTIMATE2012',
        'POPESTIMATE2013',
        'POPESTIMATE2014',
        'POPESTIMATE2015']
df.apply(lambda x: np.max(x[rows]), axis=1)

=====
# Group by
for group, frame in df.groupby('STNAME'):
    avg = np.average(frame['CENSUS2010POP'])
    print('Counties in state ' + group + ' have an average population of ' + str(avg))
==>group fn, groupby stname, avg = avg frame census2010pop, print out result
==========

df = df.set_index('STNAME')

def fun(item):
    if item[0]<'M': #all states before letter m
        return 0
    if item[0]<'Q': #all states before letter q
        return 1
    return 2

for group, frame in df.groupby(fun):
    print('There are ' + str(len(frame)) + ' records in group ' + str(group) + ' for processing.')

set index 1st, use group fn & fun (we define) fn to segmet data. typically usued to reistribute work.

===============

df.groupby('STNAME').agg({'CENSUS2010POP': np.average})
==> this is to group, then apply a fun across a column; gives the avg census.. over the states; similar to r's tapply fn.

==> finally did it
print(df)

import numpy as np

# Your code here
df['Total']= df['Weight (oz.)']*df['Quantity']

#df['Total']= 'Weight (oz.)'*int('Quantity')

print(df)


#def a(Weight):
 
# Total = Weight*Quantity
  
#return Total 
  
df.groupby('Category').agg({'Total': np.sum})


========////
(df.set_index('STNAME').groupby(level=0)['CENSUS2010POP']
    .agg({'avg': np.average, 'sum': np.sum}))

#series groupy above
#dfgroupby below
(df.set_index('STNAME').groupby(level=0)['POPESTIMATE2010','POPESTIMATE2011']
    .agg({'avg': np.average, 'sum': np.sum}))

# NOTE !!!!!!!!!!!!!!
(df.set_index('STNAME').groupby(level=0)['POPESTIMATE2010','POPESTIMATE2011']
    .agg({'POPESTIMATE2010': np.average, 'POPESTIMATE2011': np.sum}))

==>do not pass the col name again,same name in the .agg field as pandas will apply function(np.sum in this case) to the col directly and not create a new hieririchal col
==
#scales 
categor/nominal data
df['Grades'].astype('category').head()
grades = df['Grades'].astype('category',
                             categories=['D', 'D+', 'C-', 'C', 'C+', 'B-', 'B', 'B+', 'A-', 'A', 'A+'],
                             ordered=True)
why we order, cus ordered data has boolean masking true and false, so we can make coparisons suh as
grades > 'C'

===
# reduce ratio/inteval data to categorical data, esp for things like histograms.
the fn cut does that, takes in no of bins which are equally spaced.

df = pd.read_csv('census.csv')
df = df[df['SUMLEV']==50]
df = df.set_index('STNAME').groupby(level=0)['CENSUS2010POP'].agg({'avg': np.average})
pd.cut(df['avg'],10)

# You can also add labels for the sizes [Small < Medium < Large].

pd.cut(s, 3, labels=['Small', 'Medium', 'Large'])
===============

#pivot tables
df.pivot_table(values='(kW)', index='YEAR', columns='Make', aggfunc=np.mean)
==> value is what we are trying to get the mean of..
df.pivot_table(values='(kW)', index='YEAR', columns='Make', aggfunc=[np.mean,np.min], margins=True)
=>margins true, will show overall min/max at the end of the table
->pass any fn to the ..add fn, includ those i define myself

ttest=
# Your code here  -  index='YEAR'

print(Bikes.pivot_table(values='Price', columns=('Bike Type','Manufacturer'), aggfunc=np.mean))


print(Bikes.pivot_table(values='Price', columns=('Manufacturer','Bike Type'), aggfunc=np.mean))

solu=
print(pd.pivot_table(Bikes, index=['Manufacturer','Bike Type']))
===

# Date Functionality in Pandas
>>>>>Timestamp
pd.Timestamp('9/1/2016 10:05AM')


>>>>>>>Period
pd.Period('1/2016')
Period('2016-01', 'M')

pd.Period('3/5/2016')
Period('2016-03-05', 'D')

>>>>>>DatetimeIndex

t1 = pd.Series(list('abc'), [pd.Timestamp('2016-09-01'), pd.Timestamp('2016-09-02'), pd.Timestamp('2016-09-03')])
t1
2016-09-01    a
2016-09-02    b
2016-09-03    c
dtype: object

type(t1.index)
pandas.tseries.index.DatetimeIndex

>>>>>>>PeriodIndex
t2 = pd.Series(list('def'), [pd.Period('2016-09'), pd.Period('2016-10'), pd.Period('2016-11')])
t2
2016-09    d
2016-10    e
2016-11    f
Freq: M, dtype: object
pandas.tseries.period.PeriodIndex

### Converting to Datetime
d1 = ['2 June 2013', 'Aug 29, 2014', '2015-06-26', '7/12/16']
ts3 = pd.DataFrame(np.random.randint(10, 100, (4,2)), index=d1, columns=list('ab'))
ts3

		a	b
2 June 2013	19	39
Aug 29, 2014	54	84
2015-06-26	75	58
7/12/16		46	27

ts3.index = pd.to_datetime(ts3.index)
ts3
==>change data format to a good order
		a	b
2013-06-02	19	39
2014-08-29	54	84
2015-06-26	75	58
2016-07-12	46	27

### Timedeltas - are differences in time

pd.Timestamp('9/3/2016')-pd.Timestamp('9/1/2016')
Timedelta('2 days 00:00:00')

pd.Timestamp('9/2/2016 8:10AM') + pd.Timedelta('12D 3H')
Timestamp('2016-09-14 11:10:00')

###Working with Dates in a Dataframe

dates = pd.date_range('10-01-2016', periods=9, freq='2W-SUN')
dates

DatetimeIndex(['2016-10-02', '2016-10-16', '2016-10-30', '2016-11-13',
               '2016-11-27', '2016-12-11', '2016-12-25', '2017-01-08',
               '2017-01-22'],
              dtype='datetime64[ns]', freq='2W-SUN')

df = pd.DataFrame({'Count 1': 100 + np.random.randint(-5, 10, 9).cumsum(),
                  'Count 2': 120 + np.random.randint(-5, 10, 9)}, index=dates)
df

		Count 1	Count 2
2016-10-02	104	125
2016-10-16	109	122
2016-10-30	111	127
2016-11-13	117	126
2016-11-27	114	126
2016-12-11	109	121
2016-12-25	105	126
2017-01-08	105	125
2017-01-22	101	123


df.index.weekday_name = chk day of the wee date is on
array(['Sunday', 'Sunday', 'Sunday', 'Sunday', 'Sunday', 'Sunday',
       'Sunday', 'Sunday', 'Sunday'], dtype=object)

df.diff() - diff btw each dates value.
df.resample('M').mean() - mean each month in sample

******partial indexing to find info from a particulatr set. e,g yr df['2017'] or month df['2016-12'] or range df['2016-12':]

df.asfreq('W', method='ffill') - change freq of dates in our table to weekly we will get missing vales, so we use forward fill in this code

==>visalize timeseries
import matplotlib.pyplot as plt
%matplotlib inline

df.plot()
================================ss

### Distributions in Pandas

np.random.binomial(1000, 0.5)/1000 - run random binom generator 1 or 0,prob set to .5 - siginifies head or tail
we want to simulate the probability of flipping a fair coin 20 times, and getting a number greater than or equal to 15. Use np.random.binomial(n, p, size) to do 10000 simulations of flipping a fair coin 20 times
print(sum((np.random.binomial(20, 0.5,10000))>15))

chance_of_tornado = 0.01

tornado_events = np.random.binomial(1, chance_of_tornado, 1000000)
    
two_days_in_a_row = 0
for j in range(1,len(tornado_events)-1):
    if tornado_events[j]==1 and tornado_events[j-1]==1:
        two_days_in_a_row+=1

print('{} tornadoes back to back in {} years'.format(two_days_in_a_row, 1000000/365))
===============
distri=
np.random.uniform(0, 1)
np.random.normal(0.75)   or gaussian

distribution = np.random.normal(0.75,size=1000)
np.sqrt(np.sum((np.mean(distribution)-distribution)**2)/len(distribution))
np.std(distribution)

======>shape of tail of distribution   -ve vale means curve is slightly ore flat than normal distrib; +ve meas slightly more peaky than normal distribution

import scipy.stats as stats
stats.kurtosis(distribution)

stats.skew(distribution) - nt much sew as ge with norm distr template/desig

chi_squared_df2 = np.random.chisquare(2, size=10000) #has 1 parameter degrees of freedom,leftskewed

stats.skew(chi_squared_df2)

chi_squared_df5 = np.random.chisquare(5, size=10000)
stats.skew(chi_squared_df5)
#as we increase the degrees of freed, the skew moves to the cetre, we chck plot with a map below:

%matplotlib inline
import matplotlib
import matplotlib.pyplot as plt

output = plt.hist([chi_squared_df2,chi_squared_df5], bins=50, histtype='step', 
                  label=['2 degrees of freedom','5 degrees of freedom'])
plt.legend(loc='upper right')

===
from scipy import stats
stats.ttest_ind?
=> test used for comparig info from a statisti df

stats.ttest_ind(early['assignment1_grade'], late['assignment1_grade'])
Ttest_indResult(statistic=1.400549944897566, pvalue=0.16148283016060577)
=> pvalue higher than set 0.05 so we can not igore as it shows evidce of alternative hypo


=========================
%matplotlib notebook - used to render plots to screens/ jupyter notebookk  - mtp 
import matplotlib as mpl
mpl.get_backend()

# we can pass in '.' to plt.plot to indicate that we want
# the point (3,2) to be indicated with a marker '.'
plt.plot(3, 2, '.')


===
# create a new figure - @as we put new fig, we plot new fig, ignore old one
plt.figure()

# plot the point (3,2) using the circle marker
plt.plot(3, 2, 'o')

# get the current axes
ax = plt.gca()
======================

# get current axes
ax = plt.gca()
# get all the child objects the axes contains
ax.get_children()
==>
[<matplotlib.lines.Line2D at 0x7fb48589bb70>,
 <matplotlib.lines.Line2D at 0x7fb4858eaa20>,
 <matplotlib.lines.Line2D at 0x7fb48589bcf8>,
 <matplotlib.spines.Spine at 0x7fb4858ed080>,
 <matplotlib.spines.Spine at 0x7fb485972198>,
 <matplotlib.spines.Spine at 0x7fb4859e2208>,
 <matplotlib.spines.Spine at 0x7fb48585f390>,
 <matplotlib.axis.XAxis at 0x7fb48585f208>,
 <matplotlib.axis.YAxis at 0x7fb4858cb470>,
 <matplotlib.text.Text at 0x7fb48587bdd8>,
 <matplotlib.text.Text at 0x7fb48587be48>,
 <matplotlib.text.Text at 0x7fb48587beb8>,
 <matplotlib.patches.Rectangle at 0x7fb48587bef0>]


 One, pyplot is going to retrieve the current figure with the function gcf and then get the current axis with the function gca. 
0:42
Pyplot is keeping track of the axis objects for you. But don't forget that they're there and we can get them when we want to get them. 
0:49
Two, also pyplot just mirrors the API of the axis objects. So you can call the plot function against the pyplot module. But this is calling the axis plot functions underneath, so be aware. 
1:03
And three, finally, remember that the function declaration from most of the functions in matplotlib end with an open set of keyword arguments.
=============///////////////////

# Set axis properties [xmin, xmax, ymin, ymax]
ax.axis([0,6,0,10])
=
# Scatterplots
x = np.array([1,2,3,4,5,6,7,8])
y = x

plt.figure()
plt.scatter(x, y) # similar to plt.plot(x, y, '.'), but the underlying child objects in the axes are not Line2D
========
# create a list of colors for each point to have
# ['green', 'green', 'green', 'green', 'green', 'green', 'green', 'red']
colors = ['green']*(len(x)-1)
colors.append('red')

plt.figure()

# plot the point with size 100 and chosen colors
plt.scatter(x, y, s=100, c=colors) #s is size of dots

===========

plt.figure()
# plot a data series 'Tall students' in red using the first two elements of x and y
plt.scatter(x[:2], y[:2], s=100, c='red', label='Tall students')
# plot a second data series 'Short students' in blue using the last three elements of x and y 
plt.scatter(x[2:], y[2:], s=100, c='blue', label='Short students')
# add a label to the x axis
plt.xlabel('The number of times the child kicked a ball')
# add a label to the y axis
plt.ylabel('The grade of the student')
# add a title
plt.title('Relationship between ball kicking and grades')
=====

# Line Plots
import numpy as np

linear_data = np.array([1,2,3,4,5,6,7,8])
exponential_data = linear_data**2

plt.figure()
# plot the linear data and the exponential data
plt.plot(linear_data, '-o', exponential_data, '-o')
==
# fill the area between the linear data and exponential data
plt.gca().fill_between(range(len(linear_data)), 
                       linear_data, exponential_data, 
                       facecolor='blue', 
                       alpha=0.25)
===
# Bar Charts
plt.figure()
xvals = range(len(linear_data))
plt.bar(xvals, linear_data, width = 0.3)


==============
subplot -mtp
plt.figure()
# subplot with 1 row, 2 columns, and current axis is 1st subplot axes
plt.subplot(1, 2, 1)

linear_data = np.array([1,2,3,4,5,6,7,8])

plt.plot(linear_data, '-o')
=
exponential_data = linear_data**2 

# subplot with 1 row, 2 columns, and current axis is 2nd subplot axes
plt.subplot(1, 2, 2)
plt.plot(exponential_data, '-o')
=
# to keep axis the same, use share axis
plt.figure()
ax1 = plt.subplot(1, 2, 1)
plt.plot(linear_data, '-o')
# pass sharey=ax1 to ensure the two subplots share the same y axis
ax2 = plt.subplot(1, 2, 2, sharey=ax1)
plt.plot(exponential_data, '-x')
====
# create a 3x3 grid of subplots
fig, ((ax1,ax2,ax3), (ax4,ax5,ax6), (ax7,ax8,ax9)) = plt.subplots(3, 3, sharex=True, sharey=True)
# plot the linear_data on the 5th subplot axes 
ax5.plot(linear_data, '-')
===
# set inside tick labels to visible
for ax in plt.gcf().get_axes():
    for label in ax.get_xticklabels() + ax.get_yticklabels():
        label.set_visible(True)
==
# necessary on some systems to update the plot
plt.gcf().canvas.draw()
===
#### Histograms
# create 2x2 grid of axis subplots
fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, sharex=True)
axs = [ax1,ax2,ax3,ax4]

# draw n = 10, 100, 1000, and 10000 samples from the normal distribution and plot corresponding histograms
for n in range(0,len(axs)):
    sample_size = 10**(n+1)
    sample = np.random.normal(loc=0.0, scale=1.0, size=sample_size)
    axs[n].hist(sample)
    axs[n].set_title('n={}'.format(sample_size))
===
# repeat with number of bins set to 100
fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, sharex=True)
axs = [ax1,ax2,ax3,ax4]

for n in range(0,len(axs)):
    sample_size = 10**(n+1)
    sample = np.random.normal(loc=0.0, scale=1.0, size=sample_size)
    axs[n].hist(sample, bins=100)
    axs[n].set_title('n={}'.format(sample_size))
==

# use gridspec to partition the figure into subplots
import matplotlib.gridspec as gridspec

plt.figure()
gspec = gridspec.GridSpec(3, 3)

top_histogram = plt.subplot(gspec[0, 1:])
side_histogram = plt.subplot(gspec[1:, 0])
lower_right = plt.subplot(gspec[1:, 1:])
===
Y = np.random.normal(loc=0.0, scale=1.0, size=10000)
X = np.random.random(size=10000)
lower_right.scatter(X, Y)
top_histogram.hist(X, bins=100)
s = side_histogram.hist(Y, bins=100, orientation='horizontal')
===
# clear the histograms and plot normed histograms
top_histogram.clear()
top_histogram.hist(X, bins=100, normed=True)
side_histogram.clear()
side_histogram.hist(Y, bins=100, orientation='horizontal', normed=True)
# flip the side histogram's x axis
side_histogram.invert_xaxis()
==
# change axes limits
for ax in [top_histogram, lower_right]:
    ax.set_xlim(0, 1)
for ax in [side_histogram, lower_right]:
    ax.set_ylim(-5, 5)
==
########### Box and Whisker Plots

import pandas as pd
normal_sample = np.random.normal(loc=0.0, scale=1.0, size=10000)
random_sample = np.random.random(size=10000)
gamma_sample = np.random.gamma(2, size=10000)

df = pd.DataFrame({'normal': normal_sample, 
                   'random': random_sample, 
                   'gamma': gamma_sample})

df.describe()
===
plt.figure()
# create a boxplot of the normal data, assign the output to a variable to supress output
_ = plt.boxplot(df['normal'], whis='range')
# clear the current figure
plt.clf()
# plot boxplots for all three of df's columns
_ = plt.boxplot([ df['normal'], df['random'], df['gamma'] ], whis='range')

===

import mpl_toolkits.axes_grid1.inset_locator as mpl_il

plt.figure()
plt.boxplot([ df['normal'], df['random'], df['gamma'] ], whis='range')
# overlay axis on top of another 
ax2 = mpl_il.inset_axes(plt.gca(), width='60%', height='40%', loc=2)
ax2.hist(df['gamma'], bins=100)
ax2.margins(x=0.5)
===
# switch the y axis ticks for ax2 to the right side
ax2.yaxis.tick_right()
=
# if `whis` argument isn't passed, boxplot defaults to showing 1.5*interquartile (IQR) whiskers with outliers
plt.figure()
_ = plt.boxplot([ df['normal'], df['random'], df['gamma'] ] )
=====
############ Heatmaps
plt.figure()

Y = np.random.normal(loc=0.0, scale=1.0, size=10000)
X = np.random.random(size=10000)
_ = plt.hist2d(X, Y, bins=25)
===
plt.figure()
_ = plt.hist2d(X, Y, bins=100)
# add a colorbar legend
plt.colorbar()
===

######### Animations

import matplotlib.animation as animation

n = 100
x = np.random.randn(n)
===
# create the function that will do the plotting, where curr is the current frame
def update(curr):
    # check if animation is at the last frame, and if so, stop the animation a
    if curr == n: 
        a.event_source.stop()
    plt.cla()
    bins = np.arange(-4, 4, 0.5)
    plt.hist(x[:curr], bins=bins)
    plt.axis([-4,4,0,30])
    plt.gca().set_title('Sampling the Normal Distribution')
    plt.gca().set_ylabel('Frequency')
    plt.gca().set_xlabel('Value')
    plt.annotate('n = {}'.format(curr), [3,27])
===========

fig = plt.figure()
a = animation.FuncAnimation(fig, update, interval=100)
=====
########## Interactivity
plt.figure()
data = np.random.rand(10)
plt.plot(data)

def onclick(event):
    plt.cla()
    plt.plot(data)
    plt.gca().set_title('Event at pixels {},{} \nand data {},{}'.format(event.x, event.y, event.xdata, event.ydata))

# tell mpl_connect we want to pass a 'button_press_event' into onclick when the event is detected
plt.gcf().canvas.mpl_connect('button_press_event', onclick)
==
from random import shuffle
origins = ['China', 'Brazil', 'India', 'USA', 'Canada', 'UK', 'Germany', 'Iraq', 'Chile', 'Mexico']

shuffle(origins)

df = pd.DataFrame({'height': np.random.rand(10),
                   'weight': np.random.rand(10),
                   'origin': origins})
df
=

plt.figure()
# picker=5 means the mouse doesn't have to click directly on an event, but can be up to 5 pixels away
plt.scatter(df['height'], df['weight'], picker=5)
plt.gca().set_ylabel('Weight')
plt.gca().set_xlabel('Height')
=
def onpick(event):
    origin = df.iloc[event.ind[0]]['origin']
    plt.gca().set_title('Selected item came from {}'.format(origin))

# tell mpl_connect we want to pass a 'pick_event' into onpick when the event is detected
plt.gcf().canvas.mpl_connect('pick_event', onpick)
=



def onpick(event):
    origin = df.iloc[event.ind[0]]['origin']
    plt.gca().set_title('Selected item came from {}'.format(origin))

# tell mpl_connect we want to pass a 'pick_event' into onpick when the event is detected
plt.gcf().canvas.mpl_connect('pick_event', onpick)
def onpick(event):
    origin = df.iloc[event.ind[0]]['origin']
    plt.gca().set_title('Selected item came from {}'.format(origin))

# tell mpl_connect we want to pass a 'pick_event' into onpick when the event is detected
plt.gcf().canvas.mpl_connect('pick_event', onpick)

======
t.append(x)
t = t + [x]
t += [x]
And these are wrong:
t.append([x]) # WRONG!
t = t.append(x) # WRONG!
t + [x] # WRONG!
t = t + x # WRONG!
===
reduce: A processing pattern that traverses a sequence and accumulates the elements into
a single result.
map: A processing pattern that traverses a sequence and performs an operation on each
element.
=================
To see whether something appears as a value in a dictionary, you can use the method
values, which returns a collection of values, and then use the in operator:
>>> vals = eng2sp.values()
>>> 'uno' in vals
True
====
A previously computed value that is stored for later use is called a memo #hash // dict
===
To reassign a global variable inside a function you have to declare the global variable before you use it:

been_called = False

def example2():
	global been_called
	been_called = True

The global statement tells the interpreter something like, “In this function, when I say
been_called, I mean the global variable; don’t create a local one.
=================
If a global variable refers to a mutable value, you can modify the value without declaring
the variable:
known = {0:0, 1:1}
def example4():
known[2] = 1
So you can add, remove and replace elements of a global list or dictionary, but if you want
to reassign the variable, you have to declare it:
def example5():
global known
known = dict()
===============









week 4 ===============================

























===========

https://ae.linkedin.com/in/kirill-smolyakov-5751b8145